include : 'Importer'

# load configuration
configfile : 'config/config.yaml'

from utils import parse_hmmsearch
from utils import bipartition_compatibility, bipartition_compatibility_ratio
from utils import find_links
from utils import batched

import os, pandas, polars, dendropy
from pathlib import Path
from copy import deepcopy
from hmm_profile import reader as hmmreader
from hmm_profile import writer as hmmwriter
from cdhit_reader import read_cdhit
from Bio.SeqIO import parse
from Bio import AlignIO, SearchIO
from Bio.Align import MultipleSeqAlignment
from numpy import linspace, zeros, unique
from collections import defaultdict
from itertools import combinations, product
from scipy.cluster.hierarchy import single, fcluster
from scipy.spatial.distance import squareform
from scipy.stats import kendalltau, pearsonr, skew, kurtosis
from statistics import stdev
from SuchTree import SuchTree, SuchLinkedTrees

HMMS_DIR    = config['hmm_dir']
GENOMES_DIR = config['genome_dir']

# index data inputs
HMM_FILES    = glob_wildcards( os.path.join( HMMS_DIR, '{hmm}.hmm' ) )
GENOME_FILES = glob_wildcards( os.path.join( GENOMES_DIR, '{fasta}.fna' ) )

# aggregate genomes into chunks
CHUNKS = [ 'chunk_{i}'.format(i=str(i)) for i in range( workflow.cores ) ]

# extract the {hmm} values into a list
HMMS    = HMM_FILES.hmm

# extract the {fasta} values into a list
GENOMES = GENOME_FILES.fasta

# genome fasta paths
GENOME_PATHS = { genome : os.path.join( GENOMES_DIR, genome + '.fna' ) 
                 for genome in GENOMES }

# reference and query set for fastANI
R = GENOMES[:len(GENOMES)//2]
Q = GENOMES[len(GENOMES)//2:]

# gene pair lists
HMMS1,HMMS2 = list( zip( *combinations( HMMS, 2 ) ) )

# e-value threshold for HMM search
EVALUE = config['evalue']

# post-trimming aligned fraction threshold
AFT = config['aligned_fraction_threshold']

# threads to use when invoking multithreaded applications
THREADS = config['threads']

# cutoff thresholds for genome ANI clustering
ANI_THRESHOLDS = linspace( config['ani_thresholds_min'],
                           config['ani_thresholds_max'],
                           config['ani_thresholds_n'] )

# minimum number of links per cluster for tree correlation
MIN_CLUSTER_LINKS = config['min_cluster_links']

# degenerate tree threshold : exclude pairwise tree-to-tree
# comparisons where the ratio of non-unique leaf-to-leaf
# distances exceeds this threshold
DEGEN_THRESHOLD = config['degeneracy_threshold']

rule all :
  input :
    'cluster_filtering/clusters.tsv'

rule clean :
  shell :
    'rm -rf proteins genes hmm_hits hmmdb \
        preani ani ani_clusters scored_proteins \
        cluster_filtering clusters statistics \
        benchmarks logs'

rule hmmr_summary :
  input :
    genomes    = expand( 'data/genomes-enabled/{fasta}.fna', fasta=GENOMES ),
    proteins   = expand( 'proteins/{fasta}_genes.faa', fasta=GENOMES ),
    hits       = expand( 'hmm_hits/{fasta}_prot_tbl.txt', hmm=HMMS, fasta=GENOMES ),
  output :
    'statistics/hmmr_summary.txt'
  log :
    'logs/statistics/hmmr_summary.log'
  run :
    attribs = [ 'accession', 'bias', 'bitscore', 'description',
                'cluster_num', 'domain_exp_num', 'domain_included_num',
                'domain_obs_num', 'domain_reported_num', 'env_num',
                'evalue', 'id', 'overlap_num', 'region_num']
    
    hitlist = {}
    
    for hmm_hits in input.hits :
      hits = defaultdict(list)
      with open( hmm_hits ) as handle :
        for queryresult in SearchIO.parse( handle, 'hmmer3-tab' ) :
          for hit in queryresult.hits :
            for attrib in attribs :
              hits[attrib].append( getattr(hit, attrib) )
            hitlist[hit.id] = { 'hmm'      : queryresult.id,
                                'bitscore' : hit.bitscore }
    
    # Note : counting proteins from 1 instead of 0 because that's how
    # prodigal does it. Other counts are from 0.
    
    with open( output[0], 'w' ) as f :
      for fna,faa in list( zip( input.genomes, input.proteins ) ) :
        f.write( fna + '\n')
        contigs  = [ rec.id for rec in parse( open( fna ), 'fasta' ) ]
        proteins = [ rec.id for rec in parse( open( faa ), 'fasta' ) ]
        annot = {}
        for r in proteins :
          contig = r.rsplit('_', 1)[0]
          if not contig in annot :
            annot[contig] = []
          annot[contig].append( r )
        for n,contig in enumerate( contigs ) :
          f.write( '   contig {n} : {c} '.format( n=n, c=contig ) + '\n' )
          for m,protein in enumerate( annot[contig] ) :
            if protein in hitlist :
              f.write( '      protien {m} : {p}\n'.format( m=m+1, p=protein ) )
              f.write( '         hmm : {h} bitscore {b}\n'.format( h=hitlist[protein]['hmm'],
                                                                   b=hitlist[protein]['bitscore'] ) )
            else :
              f.write( '      protien {m} : {p}\n'.format( m=m+1, p=protein ) )

rule aggregate :
  group : 'gene_prediction'
  input :
    fastas=expand( 'data/genomes-enabled/{fasta}.fna', fasta=GENOMES ),
  output :
    chunks=expand( 'aggregated_genomes/{chunk}.fna', chunk=CHUNKS ),
  log :
    'logs/aggregate/aggregate.log',
  run :
    with open( log[0], 'w' ) as LOG :
      fasta_chunks = batched( input.fastas, size=len(input.fastas)//workflow.cores )
      if not os.path.exists( 'aggregated_genomes' ) : os.mkdir( 'aggregated_genomes' )
      for n,chunk in enumerate( fasta_chunks ) :
        chunkpath = os.path.join( 'aggregated_genomes',
                                  'chunk_{n}.fna'.format(n=str(n) ) ) 
        LOG.write( 'writing {chunk}...\n'.format( chunk=chunkpath ) )
        with open( chunkpath, 'w' ) as f :
          for genome in chunk :
            for i,seq in enumerate( parse( open( genome ), 'fasta' ) ) :
              seq.id = Path(genome).stem + '_contig_' + str(i)
              seq.description = ''
              f.write( seq.format('fasta') )
            LOG.write( '   wrote {i} genomes'.format( i=str(i) ) )

rule prodigal :
  group : 'gene_prediction'
  input :
    fna='aggregated_genomes/{chunk}.fna', 
  output :
    faa='proteins/{chunk}.faa',
    fna='genes/{chunk}.fna',
  log :
    'logs/prodigal/{chunk}.log',
  benchmark :
    'benchmarks/prodigal/{chunk}.tsv'
  params :
    extra='',
  wrapper :
    'file://wrappers/prodigal-gv'

rule hmmdb :
  group : 'gene_prediction'
  input :
    hmms = expand( 'data/hmms-enabled/{hmm}.hmm', hmm=HMMS )
  output :
    db   = 'hmmdb/models.hmm'
  log :
    'logs/hmmdb/hmmdb.log'
  benchmark :
    'benchmarks/hmmdb/hmmdb.tsv'
  run :
    profiles = []
    for hmmfile in input.hmms :
      hmm = hmmreader.read_single( open( hmmfile ) )
      if Path(hmmfile).stem != hmm.metadata.model_name :
        raise Exception( 'file name ({hmmfile}) must match model name ({modelname}).'.format(
                          hmmfile=hmmfile, modelname=hmm.metadata.model_name ) )
      profiles.append( hmm ) 
    hmmwriter.save_many_to_file( hmms=profiles, output=output.db )

rule hmmer :
  group : 'gene_prediction'
  input :
    fasta   = 'proteins/{chunk}.faa',
    profile = 'hmmdb/models.hmm'
  log :
    'logs/hmmer/{chunk}.log',
  benchmark :
    'benchmarks/hmmer/{chunk}.tsv'
  output :
    tblout         = 'hmm_hits/{chunk}_prot_tbl.txt',
    #domtblout      = 'hmm_hits/{fasta}_{hmm}_prot_dom_tbl.txt',
    #alignment_hits = 'hmm_hits/{fasta}_{hmm}_alg_hits.txt',
    #outfile        = 'hmm_hits/{fasta}_{hmm}_prot.txt'
  wrapper :
    'file://wrappers/hmmer/hmmsearch'

rule scoring :
  input :
    # for each HMM...
    hits    = expand( 'hmm_hits/{chunk}_prot_tbl.txt', chunk=CHUNKS ),
    # ... score orthologs for each genome...
    faa     = expand( 'proteins/{chunk}.faa', chunk=CHUNKS ),
    profile = 'hmmdb/models.hmm'
  output :
    prealign = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS )
  log :
    'logs/scoring/scoring.log'
  benchmark :
    'benchmarks/scoring/scoring.tsv'
  run :
    with open( log[0], 'w' ) as LOG :
      prealignments = { Path(p).stem : { 'prealign' : p,
                                         'hits'     : [] } for p in output.prealign }
      # use trusted cutoffs if provided
      trusted_cutoffs = { m.metadata.model_name : m.metadata.trusted_cutoff
                          for m in reader.read_all( open( input.profile ) ) }
      LOG.write( 'parsed {n} hmms, found {m} trusted cutoffs\n'.format( 
                    n=str(len(trusted_cutoffs)),
                    m=str(sum( [ bool(v) for v in trusted_cutoffs.values() ] ) ) ) )
      for tblout,faa in zip( input.hits, input.faa ) :
        hits = pandas.DataFrame( [ h for h in parse_hmmsearch( tblout ) ] )
        seqs = { seq.id : seq for seq in parse( open( faa ), 'fasta' ) }
        genome = Path(faa).stem.rsplit('_',1)[0]
        LOG.write( '{genome} : found {n} hits for {m} proteins\n'.format(
                      genome=genome,
                      n=str(len(hits)),
                      m=str(len(seqs)) ) )
        if len( hits ) == 0 : continue
        for hmm in set( hits['qname'] ) :
          if trusted_cutoffs[ hmm ] :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['score'] >= trusted_cutoff[hmm] ) ].sort_values( 
                                'score', axis=0, ascending=False )
          else :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['eval']  <= EVALUE ) ].sort_values( 
                                'score', axis=0, ascending=False )
          if len( top_hits ) == 0 : continue
          with open( prealignments[ hmm ]['prealign'], 'a' ) as f :
            for i,(n,row) in enumerate( top_hits.iterrows() ) :
              LOG.write( '   {hmm} {n} : {tname}\n'.format( hmm=hmm, n=str(i), tname=row['tname'] ) )
              seq = deepcopy( seqs[ row['tname'] ] )
              if seq.seq[-1] == '*' : seq = seq[:-1]
              seq.id = seq.id + '_p' + str(i)
              prealignments[hmm]['hits'].append( seq.id )
              f.write( seq.format( 'fasta' ) )
      for k,p in prealignments.items() :
        LOG.write( '{hmm} : \n'.format( hmm=k ) )
        for hit in p['hits'] :
          LOG.write( '    {hit}\n'.format( hit=hit ) )
        if len( p['hits'] ) == 0 :
          LOG.write( '    NONE\n' )
          with open( p['prealign'], 'w' ) as f :
            f.write( '' )

rule preani :
  group : 'ani'
  input :
    references = expand( 'data/genomes-enabled/{reference}.fna', reference=R ),
    queries    = expand( 'data/genomes-enabled/{query}.fna', query=Q )
  output :
    references = 'preani/references.txt',
    queries    = 'preani/queries.txt',
  log :
    'logs/preani/preani.log'
  benchmark :
    'benchmarks/preani/preani.tsv'
  run :
    with open( output.references, 'w' ) as rf :
      for fasta in input.references :
        rf.write( fasta + '\n' )
    with open( output.queries, 'w' ) as qf :
      for fasta in input.queries :
        qf.write( fasta + '\n' )

rule fastani :
  group : 'ani'
  input :
    reference = 'preani/references.txt',
    query     = 'preani/queries.txt'
  output :
    'ani/ani.tsv'
  log :
    'logs/fastani/fastani.log'
  benchmark :
    'benchmarks/fastani/fastani.tsv'
  threads : THREADS
  wrapper :
    'file://wrappers/fastani'

rule ani_clusters :
  group : 'ani'
  input :
    ani = 'ani/ani.tsv',
  output :
    'ani_clusters/ani_clusters_{ani_cutoff}.txt'
  log :
    'logs/ani_clusters/{ani_cutoff}.log'
  benchmark :
    'benchmarks/ani_clusters/{ani_cutoff}.tsv'
  run :
    header = [ 'query', 'reference', 'ANI', 
               'bidirectional fragment mappings',
               'total query fragments' ]
    
    fastani = pandas.read_csv( input.ani, sep='\t', names=header )
    
    # file names -> genome names
    fastani['query']     = [ Path(p).stem for p in fastani['query'] ]
    fastani['reference'] = [ Path(p).stem for p in fastani['reference'] ]
    
    # drop self-hits
    fastani = fastani[ fastani['query'] != fastani['reference'] ]
    
    # de-duplicate (i.e., take the lower triangle of the matrix)
    fastani['hash'] = fastani.apply( lambda row : hash( 
                                                    tuple(
                                                      sorted( ( row['query'],
                                                                row['reference'] ) ) ) ), 
                                    axis=1 )

    fastani.drop_duplicates( subset='hash', keep='first', inplace=True )
    fastani.drop( ['hash'], axis=1, inplace=True )
    
    # convert % identity to a distance measure for clustering
    threshold = 1.0 - float( Path( output[0] ).stem.rsplit( '_', 1 )[1] )/100
    
    with open( output[0], 'w' ) as f :
      genomes = list( set( fastani['query'] ) | set( fastani['reference'] ) )
    
      D = zeros( ( len(genomes), len(genomes) ) )
      D.fill( 1.0 )
        
      for i in range(len(genomes)) :
        D[i,i] = 0.0
        
      for n,row in fastani.iterrows() :
        i = genomes.index( row['query'] )
        j = genomes.index( row['reference'] )
        D[i,j] = D[j,i] = 1.0 - row['ANI']/100
         
      ani_clusters = defaultdict(list)
      for n,c in enumerate( fcluster( single( squareform(D) ),
                                      threshold,
                                      criterion='distance' ) ) :
        ani_clusters[c].append( genomes[n] )
        
      for cid, cg in ani_clusters.items() :
        f.write( '{cid}\t{genomes}\n'.format( cid=str(cid), genomes=','.join(cg) ) )

rule build_clusters :
  input :
    prealignments = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS ),
    genes         = expand( 'genes/{chunk}.fna', chunk=CHUNKS ),
    ani_clusters  = expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS )
  output :
    cluster_table = 'cluster_filtering/clusters.tsv',
    clusters      = directory( 'clusters' ),
  params :
  log :
    'logs/cluster_filtering/cluster_filtering.log'
  benchmark :
    'benchmarks/cluster_filtering/cluster_filtering.tsv'
  run :
    def intersect( hmms, genomes, threshold ) :
      C = defaultdict( set )
      for hmm,genome in zip( hmms, genomes ) :
        C[hmm].add(genome)
      return sum( [ len( C[a] & C[b] ) > threshold
                    for a,b in combinations( C.keys(), 2 ) ] )
    
    with open( log[0], 'w' ) as LOG :
      LOG.write( 'loading sequences...\n' )
      sequences = { Path(fasta).stem : { rec.id : rec.seq for rec in parse( open( fasta ), 'fasta' ) } 
                    for fasta in input.prealignments }
      
      geneseqs = { Path(fasta).stem.rsplit('_',1)[0] : { rec.id : rec
                                        for rec in parse( open( fasta ), 'fasta' ) }
                   for fasta in input.genes }
      
      LOG.write( 'Genomes loaded :\n' )
      for genome in geneseqs.keys() :
        LOG.write( '    {genome} : {n}\n'.format( genome=genome, n=len(geneseqs[genome]) ) )
      
      LOG.write( 'Genes loaded :\n' )
      for hmm in sequences.keys() :
        LOG.write( '    {hmm} : {n}\n'.format( hmm=hmm, n=len(sequences[hmm]) ) )
      
      LOG.write( 'creating gene table for genome clusters...\n' )
      records = []
      for clusterfile in input.ani_clusters :
        for cid,names in [ line.split() for line in open( clusterfile ) ] :
          names = names.split(',')
          #if not len( names ) > 5 : continue
          for hmm in sequences.keys() :
            seq_ids = [ seq_id for seq_id in sequences[hmm].keys() if seq_id.rsplit('_',3)[0] in names ]
            for seq_id in seq_ids :
              records.append( { 'seq_id' : seq_id,
                                'genome' : seq_id.rsplit('_',3)[0],
                                'cid'    : int( cid ),
                                'hmm'    : hmm,
                                'ANI'    : float( Path( clusterfile ).stem.split('_')[-1] ),
                                'id'     : cid + '_' + Path( clusterfile ).stem.split('_')[-1] } )
      
      df = polars.DataFrame( records )
      
      LOG.write( 'number of clusters by ANI...\n' )
      for ani,n in df.groupby('ANI').agg( polars.col('cid').unique().count() ).sort('ANI').iter_rows() :
        LOG.write( '    ANI={ani} : {n} clusters\n'.format( ani=ani, n=n ) )
      
      LOG.write( 'gene table description :\n\n' + str( df.describe() ) + '\n' )
      
      clusters = df.groupby('id').agg( ['hmm','genome'] ).apply(
                         lambda x : ( x[0],
                           intersect( x[1],
                                      x[2],
                                      MIN_CLUSTER_LINKS ) ) ).filter( polars.col('column_1') > 0 )['column_0']
      
      LOG.write( 'found {n} genome clusters with at least {T} genomes sharing at least two genes.\n'.format(
                 n=len(clusters), T=MIN_CLUSTER_LINKS ) )
      
      cluster_table = df.filter( polars.col('id').is_in( clusters ) )
      
      LOG.write( 'filtered gene table description :\n\n' + str( cluster_table.describe() ) )

      cluster_table.write_csv( file=output.cluster_table, separator='\t' )

      # create cluster directory structure and write gene nucleotide
      # sequences as individual fastas per cluster
      
      LOG.write( 'found {n} clusters.\n'.format( n=len( cluster_table.groupby('id').agg('seq_id') ) ) )
      
      if not os.path.exists( 'clusters' ) : os.mkdir( 'clusters' )

      for ANI,cids in cluster_table.groupby('ANI').agg( polars.col('cid').unique() ).sort('ANI').iter_rows() :
        
        anipath = os.path.join( 'clusters', str(ANI) )
        if not os.path.exists( anipath ) : os.mkdir( anipath )
        
        for cluster_id in cids :
          
          clusterpath = os.path.join( anipath, str( cluster_id ) )
          if not os.path.exists( clusterpath ) : os.mkdir( clusterpath )
          genepath = os.path.join( clusterpath, 'genes' )
          if not os.path.exists( genepath ) : os.mkdir( genepath )
          
          for hmm,seq_ids,genomes in cluster_table.filter( ( polars.col('ANI') == ANI ) & ( polars.col('cid') == cluster_id ) ).groupby('hmm').agg( ['seq_id', 'genome'] ).iter_rows() :
            with open( os.path.join( genepath, hmm + '.fna' ), 'w' ) as f :
              for seq_id,genome in zip( seq_ids, genomes ) :
                orig_seq_id = seq_id.rsplit('_',1)[0] # remove hmmer evalue rank
                seq = geneseqs[genome][orig_seq_id] 
                seq.id = seq_id
                f.write( seq.format( 'fasta' ) )
