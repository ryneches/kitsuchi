# load configuration
configfile : 'config/config.yaml'

import os, pandas, polars, dendropy
from pathlib import Path
from copy import deepcopy
from hmm_profile import reader, writer
from cdhit_reader import read_cdhit
from Bio.SeqIO import parse
from Bio import AlignIO, SearchIO
from Bio.Align import MultipleSeqAlignment
from numpy import linspace, zeros, unique
from collections import defaultdict
from itertools import combinations
from scipy.cluster.hierarchy import single, fcluster
from scipy.spatial.distance import squareform
from scipy.stats import kendalltau, pearsonr, skew, kurtosis
from statistics import stdev
from SuchTree import SuchTree, SuchLinkedTrees

HMMS_DIR    = 'data/hmms-enabled'
GENOMES_DIR = 'data/genomes-enabled'

# index data inputs
HMM_FILES    = glob_wildcards( os.path.join( HMMS_DIR, '{hmm}.hmm' ) )
GENOME_FILES = glob_wildcards( os.path.join( GENOMES_DIR, '{fasta}.fna' ) )

# extract the {hmm} values into a list
HMMS    = HMM_FILES.hmm

# extract the {fasta} values into a list
GENOMES = GENOME_FILES.fasta

# genome fasta paths
GENOME_PATHS = { genome : os.path.join( GENOMES_DIR, genome + '.fna' ) 
                 for genome in GENOMES }

# reference and query set for fastANI
R = GENOMES[:len(GENOMES)//2]
Q = GENOMES[len(GENOMES)//2:]

# gene pair lists
HMMS1,HMMS2 = list( zip( *combinations( HMMS, 2 ) ) )

# cluster checkpoint wildcards
CKANIS = None
CKCIDS = None
CKHMMS = None

# e-value threshold for HMM search
EVALUE = 10e-3

# post-trimming aligned fraction threshold
AFT = 0.75

# threads to use when invoking multithreaded applications
THREADS=1

# cutoff thresholds for genome ANI clustering
ANI_THRESHOLDS = linspace( 75, 99, 25 )

# minimum number of links per cluster for tree correlation
MIN_CLUSTER_LINKS = 5

# degenerate tree threshold : exclude pairwise tree-to-tree
# comparisons where the ratio of non-unique leaf-to-leaf
# distances exceeds this threshold
DEGEN_THRESHOLD = 0.5

# borrowed from CheckV
def parse_hmmsearch( path ) :
  with open(path) as f:
    names = [ 'tname', 'qacc', 'qname', 'tacc',   'eval', 
              'score', 'bias', 'beval', 'bscore', 'bbias' ]
    formats = [ str,   str,   str,   str,   float, 
                float, float, float, float, float ]
    for line in f:
      if not line.startswith( '#' ) :
        values = line.split()
        yield dict( [ ( names[i], formats[i](values[i]) ) 
                      for i in range(10) ] )

def find_links( genes1, genes2 ) :
    '''
    For two sets of genes called from the same set of genomes, find the linkage
    relationships connecting the genes.
    '''
    t1_genomes = { name : name.rsplit('_', 3)[0] for name in genes1 }
    t2_genomes = { name : name.rsplit('_', 3)[0] for name in genes2 }
    
    t1_genes = defaultdict(list)
    for name in genes1 :
        genome = name.rsplit( '_', 3 )[0]
        t1_genes[genome].append( name )
    
    t2_genes = defaultdict(list)
    for name in genes2 :
        genome = name.rsplit( '_', 3 )[0]
        t2_genes[genome].append( name )
    
    links = []
    for genome in set( t1_genes.keys() ) & set( t2_genes.keys() ) :
        
        # if both genes are single copy a genome, it is linked
        # if either gene has duplications, each copy of each
        # gene is linked only if they are on the same contig
        
        if len( t1_genes[ genome] ) == len( t2_genes[ genome ] ) == 1 :
            
            links.append( ( t1_genes[genome][0], t2_genes[genome][0] ) )
            
        else :
            for gene1 in t1_genes[ genome ] :
                for gene2 in t2_genes[ genome ] :
                    contig1 = gene1.rsplit('_', 2)[0]
                    contig2 = gene2.rsplit('_', 2)[0]
                    if contig1 == contig2 :
                        links.append( ( gene1, gene2 ) )
    
    return links

def get_cluster_filenames( wildcards ) :
  ck_output = checkpoints.write_cluster_fastas.get(**wildcards).output[0]
  global CKANIS, CKCIDS, CKHMMS
  CK = glob_wildcards( os.path.join( ck_output,
                                     '{ani}',
                                     '{cid}',
                                     'genes',
                                     '{ckhmm}.fna' ) )
  CKANIS = CK.ani
  CKCIDS = CK.cid
  CKHMMS = CK.ckhmm
  
  print( 'CK :' )
  print( CK )
  
  return expand( os.path.join( ck_output,
                               '{ani}',
                               '{cid}',
                               'genes',
                               '{ckhmm}.fna' ),
                 zip,
                 ani=CKANIS,
                 cid=CKCIDS,
                 ckhmm=CKHMMS )

def get_cluster_data( wildcards ) :
  ck_output = checkpoints.write_cluster_fastas.get(**wildcards).output[0]
  global CKANIS, CKCIDS, CKHMMS
  CK = glob_wildcards( os.path.join( ck_output,
                                     '{ani}',
                                     '{cid}',
                                     'genes',
                                     '{ckhmm}.fna' ) )
  CKANIS = CK.ani
  CKCIDS = CK.cid
  CKHMMS = CK.ckhmm

  return expand( os.path.join( ck_output,
                               '{ani}',
                               '{cid}',
                               'data.tsv' ),
                 zip,
                 ani=CKANIS,
                 cid=CKCIDS )

rule all :
  input :
    #expand( 'trees/by_taxa/{hmm}.nwk', hmm=HMMS ),
    #expand( 'gene_clusters/{gene_cutoff}/{hmm}.tsv', gene_cutoff=GENE_OTU_THRESHOLDS, hmm=HMMS ),
    #expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS ),
    #expand( 'cluster_correlations/{hmm1}_{hmm2}.tsv', zip, hmm1=HMMS ),
    #'statistics/hmmr_summary.txt'
    #'cluster_correlations/cluster_correlations.tsv',
    #expand( 'clusters/{ani}/{cid}/alignments/{hmm}_aln.faa', 
    #        zip, ani=CK.ani, cid=CK.cid, hmm=CK.hmm )
    #get_cluster_filenames
    get_cluster_data
    #expand( 'clusters/{ck_ani}/{ck_cid}/data.tsv', zip, ck_ani=CK.ck_ani, ck_cid=CK.ck_cid )

rule clean :
  shell :
    'rm -rf proteins genes hmm_hits hmmdb \
        preani ani ani_clusters scored_proteins \
        cluster_filtering clusters statistics logs'

rule hmmr_summary :
  input :
    genomes    = expand( 'data/genomes-enabled/{fasta}.fna', fasta=GENOMES ),
    proteins   = expand( 'proteins/{fasta}_genes.faa', fasta=GENOMES ),
    hits       = expand( 'hmm_hits/{fasta}_prot_tbl.txt', hmm=HMMS, fasta=GENOMES ),
  output :
    'statistics/hmmr_summary.txt'
  log :
    'logs/statistics/hmmr_summary.log'
  run :
    attribs = [ 'accession', 'bias', 'bitscore', 'description',
                'cluster_num', 'domain_exp_num', 'domain_included_num',
                'domain_obs_num', 'domain_reported_num', 'env_num',
                'evalue', 'id', 'overlap_num', 'region_num']
    
    hitlist = {}
    
    for hmm_hits in input.hits :
      hits = defaultdict(list)
      with open( hmm_hits ) as handle :
        for queryresult in SearchIO.parse( handle, 'hmmer3-tab' ) :
          for hit in queryresult.hits :
            for attrib in attribs :
              hits[attrib].append( getattr(hit, attrib) )
            hitlist[hit.id] = { 'hmm'      : queryresult.id,
                                'bitscore' : hit.bitscore }
    
    # Note : counting proteins from 1 instead of 0 because that's how
    # prodigal does it. Other counts are from 0.
    
    with open( output[0], 'w' ) as f :
      for fna,faa in list( zip( input.genomes, input.proteins ) ) :
        f.write( fna + '\n')
        contigs  = [ rec.id for rec in parse( open( fna ), 'fasta' ) ]
        proteins = [ rec.id for rec in parse( open( faa ), 'fasta' ) ]
        annot = {}
        for r in proteins :
          contig = r.rsplit('_', 1)[0]
          if not contig in annot :
            annot[contig] = []
          annot[contig].append( r )
        for n,contig in enumerate( contigs ) :
          f.write( '   contig {n} : {c} '.format( n=n, c=contig ) + '\n' )
          for m,protein in enumerate( annot[contig] ) :
            if protein in hitlist :
              f.write( '      protien {m} : {p}\n'.format( m=m+1, p=protein ) )
              f.write( '         hmm : {h} bitscore {b}\n'.format( h=hitlist[protein]['hmm'],
                                                                   b=hitlist[protein]['bitscore'] ) )
            else :
              f.write( '      protien {m} : {p}\n'.format( m=m+1, p=protein ) )


rule prodigal :
  input :
    fna='data/genomes-enabled/{fasta}.fna', 
  output :
    faa='proteins/{fasta}_genes.faa',
    fna='genes/{fasta}_genes.fna',
    #gff='proteins/{fasta}_genes.gff',
  log :
    'logs/prodigal/{fasta}.log',
  params :
    extra='',
  wrapper :
    'file://wrappers/prodigal-gv'

rule hmmdb :
  input :
    hmms = expand( 'data/hmms-enabled/{hmm}.hmm', hmm=HMMS )
  output :
    db   = 'hmmdb/models.hmm'
  log :
    'logs/hmmdb/hmmdb.log'
  run :
    profiles = []
    for hmmfile in input.hmms :
      hmm = reader.read_single( open( hmmfile ) )
      if Path(hmmfile).stem != hmm.metadata.model_name :
        raise Exception( 'file name ({hmmfile}) must match model name ({modelname}).'.format(
                          hmmfile=hmmfile, modelname=hmm.metadata.model_name ) )
      profiles.append( hmm ) 
    writer.save_many_to_file( hmms=profiles, output=output.db )

rule hmmer :
  input :
    fasta   = 'proteins/{fasta}_genes.faa',
    profile = 'hmmdb/models.hmm'
  log :
    'logs/hmmer/{fasta}.log',
  output :
    tblout         = 'hmm_hits/{fasta}_prot_tbl.txt',
    #domtblout      = 'hmm_hits/{fasta}_{hmm}_prot_dom_tbl.txt',
    #alignment_hits = 'hmm_hits/{fasta}_{hmm}_alg_hits.txt',
    #outfile        = 'hmm_hits/{fasta}_{hmm}_prot.txt'
  wrapper :
    'file://wrappers/hmmer/hmmsearch'

rule scoring :
  input :
    # for each HMM...
    hits    = expand( 'hmm_hits/{fasta}_prot_tbl.txt', fasta=GENOMES ),
    # ... score orthologs for each genome...
    faa     = expand( 'proteins/{fasta}_genes.faa', fasta=GENOMES ),
    profile = 'hmmdb/models.hmm'
  output :
    prealign = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS )
  log :
    'logs/scoring/scoring.log'
  run :
    with open( log[0], 'w' ) as LOG :
      prealignments = { Path(p).stem : { 'prealign' : p,
                                         'hits'     : [] } for p in output.prealign }
      # use trusted cutoffs if provided
      trusted_cutoffs = { m.metadata.model_name : m.metadata.trusted_cutoff
                          for m in reader.read_all( open( input.profile ) ) }
      LOG.write( 'parsed {n} hmms, found {m} trusted cutoffs\n'.format( 
                    n=str(len(trusted_cutoffs)),
                    m=str(sum( [ bool(v) for v in trusted_cutoffs.values() ] ) ) ) )
      for tblout,faa in zip( input.hits, input.faa ) :
        hits = pandas.DataFrame( [ h for h in parse_hmmsearch( tblout ) ] )
        seqs = { seq.id : seq for seq in parse( open( faa ), 'fasta' ) }
        genome = Path(faa).stem.rsplit('_',1)[0]
        LOG.write( '{genome} : found {n} hits for {m} proteins\n'.format(
                      genome=genome,
                      n=str(len(hits)),
                      m=str(len(seqs)) ) )
        if len( hits ) == 0 : continue
        for hmm in set( hits['qname'] ) :
          if trusted_cutoffs[ hmm ] :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['score'] >= trusted_cutoff[hmm] ) ].sort_values( 
                                'score', axis=0, ascending=False )
          else :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['eval']  <= EVALUE ) ].sort_values( 
                                'score', axis=0, ascending=False )
          if len( top_hits ) == 0 : continue
          with open( prealignments[ hmm ]['prealign'], 'a' ) as f :
            for i,(n,row) in enumerate( top_hits.iterrows() ) :
              LOG.write( '   {hmm} {n} : {tname}\n'.format( hmm=hmm, n=str(i), tname=row['tname'] ) )
              seq = deepcopy( seqs[ row['tname'] ] )
              if seq.seq[-1] == '*' : seq = seq[:-1]
              seq.id = seq.id + '_p' + str(i)
              prealignments[hmm]['hits'].append( seq.id )
              f.write( seq.format( 'fasta' ) )
      for k,p in prealignments.items() :
        LOG.write( '{hmm} : \n'.format( hmm=k ) )
        for hit in p['hits'] :
          LOG.write( '    {hit}\n'.format( hit=hit ) )
        if len( p['hits'] ) == 0 :
          LOG.write( '    NONE\n' )
          with open( p['prealign'], 'w' ) as f :
            f.write( '' )

rule preani :
  input :
    references = expand( 'data/genomes-enabled/{reference}.fna', reference=R ),
    queries    = expand( 'data/genomes-enabled/{query}.fna', query=Q )
  output :
    references = 'preani/references.txt',
    queries    = 'preani/queries.txt',
  log :
    'logs/preani/preani.log'
  run :
    with open( output.references, 'w' ) as rf :
      for fasta in input.references :
        rf.write( fasta + '\n' )
    with open( output.queries, 'w' ) as qf :
      for fasta in input.queries :
        qf.write( fasta + '\n' )

rule fastani :
  input :
    reference = 'preani/references.txt',
    query     = 'preani/queries.txt'
  output :
    'ani/ani.tsv'
  log :
    'logs/fastani/fastani.log'
  threads : THREADS
  wrapper :
    'file://wrappers/fastani'

rule ani_clusters :
  input :
    ani = 'ani/ani.tsv',
  output :
    'ani_clusters/ani_clusters_{ani_cutoff}.txt'
  log :
    'logs/ani_clusters/{ani_cutoff}.log'
  run :
    header = [ 'query', 'reference', 'ANI', 
               'bidirectional fragment mappings',
               'total query fragments' ]
    
    fastani = pandas.read_csv( input.ani, sep='\t', names=header )
    
    # file names -> genome names
    fastani['query']     = [ Path(p).stem for p in fastani['query'] ]
    fastani['reference'] = [ Path(p).stem for p in fastani['reference'] ]
    
    # drop self-hits
    fastani = fastani[ fastani['query'] != fastani['reference'] ]
    
    # de-duplicate (i.e., take the lower triangle of the matrix)
    fastani['hash'] = fastani.apply( lambda row : hash( 
                                                    tuple(
                                                      sorted( ( row['query'],
                                                                row['reference'] ) ) ) ), 
                                    axis=1 )

    fastani.drop_duplicates( subset='hash', keep='first', inplace=True )
    fastani.drop( ['hash'], axis=1, inplace=True )
    
    # convert % identity to a distance measure for clustering
    threshold = 1.0 - float( Path( output[0] ).stem.rsplit( '_', 1 )[1] )/100
    
    with open( output[0], 'w' ) as f :
      genomes = list( set( fastani['query'] ) | set( fastani['reference'] ) )
    
      D = zeros( ( len(genomes), len(genomes) ) )
      D.fill( 1.0 )
        
      for i in range(len(genomes)) :
        D[i,i] = 0.0
        
      for n,row in fastani.iterrows() :
        i = genomes.index( row['query'] )
        j = genomes.index( row['reference'] )
        D[i,j] = D[j,i] = 1.0 - row['ANI']/100
         
      ani_clusters = defaultdict(list)
      for n,c in enumerate( fcluster( single( squareform(D) ),
                                      threshold,
                                      criterion='distance' ) ) :
        ani_clusters[c].append( genomes[n] )
        
      for cid, cg in ani_clusters.items() :
        f.write( '{cid}\t{genomes}\n'.format( cid=str(cid), genomes=','.join(cg) ) )


rule cluster_filtering :
  input :
    prealignments = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS ),
    ani_clusters  = expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS )
  output :
    clusters = 'cluster_filtering/clusters.tsv'
  params :
  log :
    'logs/cluster_filtering/cluster_filtering.log'
  run :
    def intersect( hmms, genomes, threshold ) :
      C = defaultdict( set )
      for hmm,genome in zip( hmms, genomes ) :
        C[hmm].add(genome)
      return sum( [ len( C[a] & C[b] ) > threshold
                    for a,b in combinations( C.keys(), 2 ) ] )
    
    with open( log[0], 'w' ) as LOG :
      LOG.write( 'loading sequences...\n' )
      sequences = { Path(fasta).stem : { rec.id : rec.seq for rec in parse( open( fasta ), 'fasta' ) } 
                    for fasta in input.prealignments }
      
      for hmm in sequences.keys() :
        LOG.write( '{hmm} : {n}\n'.format( hmm=hmm, n=len(sequences[hmm]) ) )
      
      LOG.write( 'creating gene table for genome clusters...\n' )
      records = []
      for clusterfile in input.ani_clusters :
        for cid,names in [ line.split() for line in open( clusterfile ) ] :
          names = names.split(',')
          #if not len( names ) > 5 : continue
          for hmm in sequences.keys() :
            seq_ids = [ seq_id for seq_id in sequences[hmm].keys() if seq_id.rsplit('_',3)[0] in names ]
            for seq_id in seq_ids :
              records.append( { 'seq_id' : seq_id,
                                'genome' : seq_id.rsplit('_',3)[0],
                                'cid'    : int( cid ),
                                'hmm'    : hmm,
                                'ANI'    : float( Path( clusterfile ).stem.split('_')[-1] ),
                                'id'     : cid + '_' + Path( clusterfile ).stem.split('_')[-1] } )
      
      df = polars.DataFrame( records )
      
      LOG.write( 'number of clusters by ANI...\n' )
      for ani,n in df.groupby('ANI').agg( polars.col('cid').unique().count() ).sort('ANI').iter_rows() :
        LOG.write( '    ANI={ani} : {n} clusters\n'.format( ani=ani, n=n ) )
      
      LOG.write( 'gene table description :\n\n' + str( df.describe() ) + '\n' )
      
      clusters = df.groupby('id').agg( ['hmm','genome'] ).apply(
                         lambda x : ( x[0],
                           intersect( x[1],
                                      x[2],
                                      MIN_CLUSTER_LINKS ) ) ).filter( polars.col('column_1') > 0 )['column_0']
      
      LOG.write( 'found {n} genome clusters with at least {T} genomes sharing at least two genes.\n'.format(
                 n=len(clusters), T=MIN_CLUSTER_LINKS ) )
      
      cluster_table = df.filter( polars.col('id').is_in( clusters ) )
      
      LOG.write( 'filtered gene table description :\n\n' + str( cluster_table.describe() ) )

      cluster_table.write_csv( file=output.clusters, sep='\t' )

checkpoint write_cluster_fastas :
  input :
    cluster_table = 'cluster_filtering/clusters.tsv',
    genes         = expand( 'genes/{fasta}_genes.fna', fasta=GENOMES )
  output :
    clusters = directory( 'clusters' )
  log :
    'logs/write_cluster_fastas/write_cluster_fastas.log'
  run :
    with open( log[0], 'w' ) as LOG :
      LOG.write( 'loading sequences...\n' )
      geneseqs = { Path(fasta).stem.rsplit('_',1)[0] : { rec.id : rec
                                        for rec in parse( open( fasta ), 'fasta' ) }
                   for fasta in input.genes }
      
      for genome in geneseqs.keys() :
        LOG.write( '{genome} : {n}\n'.format( genome=genome, n=len(geneseqs[genome]) ) )
 
      LOG.write( 'loading cluster table...\n' )
      df = polars.read_csv( input.cluster_table, sep='\t' )
      LOG.write( 'found {n} clusters.\n'.format( n=len( df.groupby('id').agg('seq_id') ) ) )

      os.mkdir( output.clusters )
      for ANI,cids in df.groupby('ANI').agg( polars.col('cid').unique() ).sort('ANI').iter_rows() :
        anipath = os.path.join( output.clusters, str(ANI) )
        os.mkdir( anipath )
        for cluster_id in cids :
          clusterpath = os.path.join( anipath, str( cluster_id ) )
          os.mkdir( clusterpath )
          genepath = os.path.join( clusterpath, 'genes' )
          os.mkdir( genepath )
          for hmm,seq_ids,genomes in df.filter( ( polars.col('ANI') == ANI ) & ( polars.col('cid') == cluster_id ) ).groupby('hmm').agg( ['seq_id', 'genome'] ).iter_rows() :
            with open( os.path.join( genepath, hmm + '.fna' ), 'w' ) as f :
              for seq_id,genome in zip( seq_ids, genomes ) :
                orig_seq_id = seq_id.rsplit('_',1)[0] # remove hmmer evalue rank
                seq = geneseqs[genome][orig_seq_id] 
                seq.id = seq_id
                f.write( seq.format( 'fasta' ) )

rule postcheck :
  input :
    get_cluster_filenames
  log :
    'logs/postcheck/{ani}_{cid}_{ckhmm}.log'
  run :
    with open( log[0], 'w' ) as LOG :
      for filename in input[0] :
        n = len( [ rec for rec in parse( open( filename ), 'fasta' ) ] )
        LOG.write( '{f} : {n}\n'.format( f=filename, n=n ) )

rule clustalo :
  input :
    #get_cluster_filenames
    'clusters/{ani}/{cid}/genes/{ckhmm}.fna'
  output :
    'clusters/{ani}/{cid}/alignments/{ckhmm}_aln.fna'
  params :
    extra=''
  log :
    'logs/clustalo/{ani}_{cid}_{ckhmm}.log'
  threads : THREADS
  wrapper :
    'file://wrappers/clustalo'

rule trimal :
  input :
    'clusters/{ani}/{cid}/alignments/{ckhmm}_aln.fna'
  output :
    'clusters/{ani}/{cid}/trimmed_alignments/{ckhmm}_aln.fna'
  log :
    'logs/trimal/{ani}_{cid}_{ckhmm}.log'
  params :
    extra='-automated1'
  wrapper :
    'file://wrappers/trimal'

rule filter_alignments :
  input :
    'clusters/{ani}/{cid}/trimmed_alignments/{ckhmm}_aln.fna'
  output :
    'clusters/{ani}/{cid}/filtered_trimmed_alignments/{ckhmm}_aln.fna'
  log :
    'logs/filter_alignments/{ani}_{cid}_{ckhmm}.log'
  run :
    if open( input[0] ).read().count( '>' ) < 3 :
      with open( log[0], 'w' ) as f :
        f.write( 'no records.' )
      with open( output[0], 'w' ) as f :
        f.write( '' )
    else :
      A = AlignIO.read( input[0], 'fasta' )
      L = A.get_alignment_length()
      B = MultipleSeqAlignment( [ s for s in A if len(s.seq.replace('-',''))/L > AFT ] )
      with open( log[0], 'w' ) as f :
        f.write( 'aligned fraction threshold : {n}\n'.format( n=AFT ) )
        f.write( 'alignment length           : {n}\n'.format( n=L ) )
        f.write( 'input alignment sequences  : {n}\n'.format( n=len(A) ) )
        f.write( 'output alignment sequences : {n}\n'.format( n=len(B) ) )
      with open( output[0], 'w' ) as f :
        if len(B) >= 3 :
          AlignIO.write( B, f, 'fasta' )
        else :
          f.write( '' )

rule fasttree :
  input :
    alignment = 'clusters/{ani}/{cid}/filtered_trimmed_alignments/{ckhmm}_aln.fna'
  output :
    tree = 'clusters/{ani}/{cid}/trees/by_gene/{ckhmm}.nwk'
  log :
    'logs/fasttree/{ani}_{cid}_{ckhmm}.log'
  params :
    extra=''
  wrapper :
    'file://wrappers/fasttree'

rule taxatrees :
  '''
  Prune trees to individual taxa using. Taxa are represented
  by the gene with the top-scoring HMM hit.
  '''
  input :
    'clusters/{ani}/{cid}/trees/by_gene/{ckhmm}.nwk',
  output :
    'clusters/{ani}/{cid}/trees/by_taxa/{ckhmm}.nwk',
  log :
    'logs/gene-taxa/{ani}_{cid}_{ckhmm}_mapping.txt',
  run :
    with open( log[0], 'w' ) as LOG :
      if not os.path.getsize( input[0] ) == 0 :
        T = dendropy.Tree.get( path=input[0],
                               schema='newick',
                               preserve_underscores=True )
        
        # Sometimes, the paralog with the lowest e-value from hmmer
        # doesn't make it through trimming and filtering. The paralog
        # with the lowest hmmer e-value AFTER alignment, trimming and
        # filtering is chosen to represent the genome in the taxa tree.
        
        G = defaultdict(dict)
        
        for leaf in T.leaf_nodes() :
          genome,contig,gene,paralog = leaf.taxon.label.rsplit( '_', 3 )
          G[genome][int(paralog[1:])] = leaf.taxon.label
        
        keep = [ G[genome][min(G[genome].keys())] for genome in G.keys() ]
        
        T.retain_taxa_with_labels( keep )

        for leaf in T.leaf_nodes() :
          taxon = leaf.taxon.label.rsplit( '_', 3 )[0]
          LOG.write( '{a}\t{b}\n'.format( a=leaf.taxon.label, b=taxon ) )
          leaf.taxon.label = taxon
        T.write( path=output[0],
                 schema='newick' )
      else :
        open( output[0], 'w' ).close()

rule clusterdata :
  input :
    trees = expand( 'clusters/{ani}/{cid}/trees/by_gene/{ckhmm}.nwk', ani=CKANIS, cid=CKCIDS, ckhmm=CKHMMS )
  output :
    table = 'clusters/{ani}/{cid}/data.tsv'
  params :
    ani   = '{ani}',
    cid   = '{cid}',
  log :
    'logs/clusterdata/{ani}_{cid}.log'
  run :
    with open( log[0], 'w' ) as LOG :
      
      LOG.write( 'ANI        : {ani}\n'.format( ani=params.ani ) )
      LOG.write( 'cluster ID : {cid}\n'.format( cid=params.cid ) )
      
      data = []
      
      for treefile1, treefile2 in combinations( input.trees, 2 ) :
        hmm1_name = Path( treefile1 ).stem.rsplit( '.', 1 )[0]
        hmm2_name = Path( treefile2 ).stem.rsplit( '.', 1 )[0]
        
        LOG.write( 'loading {g1} and {g2}...\n'.format( g1=hmm1_name, g2=hmm2_name ) )
        
        T1 = SuchTree( treefile1 )
        T2 = SuchTree( treefile2 )
        links = find_links( T1, T2 )
        
        LOG.write( '    {g1} size : {n}\n'.format( g1=hmm1_name, n=len(T1) ) )
        LOG.write( '    {g2} size : {n}\n'.format( g2=hmm2_name, n=len(T2) ) )
        LOG.write( '    links     : {n}\n'.format( n=len(links) ) )
        
        hmm1_pairs = []
        hmm2_pairs = []
        
        for (t1a,t2a),(t1b,t2b) in combinations( links, 2 ) :
          hmm1_pairs.append( ( t1a, t1b ) )
          hmm2_pairs.append( ( t2a, t2b ) )
        
        hmm1_distances = T1.distances_by_name( gene1_pairs )
        hmm2_distances = T2.distances_by_name( gene2_pairs )
        
        LOG.write( 'computed {n} distances.'.format( n=str( len( hmm1_distances ) ) ) )
        
        # skip degenerate trees
        if len( unique( hmm1_distances ) ) / len( hmm1_distances ) < DEGEN_THRESHOLD :
          LOG.write( '{g1} is degenerate, skipping.'.format( g1=hmm1_name ) )
          continue
        if len( unique( hmm2_distances ) ) / len( hmm2_distances ) < DEGEN_THRESHOLD :
          LOG.write( '{g2} is degenerate, skipping.'.format( g2=hmm2_name ) )
          continue
        
        hmm1_skew = skew( hmm1_distances )
        hmm2_skew = skew( hmm2_distances )
        
        hmm1_kurtosis = kurtosis( hmm1_distances )
        hmm2_kurtosis = kurtosis( hmm2_distances )
         
        hmm1_stdev = stdev( hmm1_distances )
        hmm2_stdev = stdev( hmm2_distances )
         
        r,pr = pearsonr(   hmm1_distances, hmm2_distances )
        t,pt = kendalltau( hmm1_distances, hmm2_distances )
            
        data.append(  { 'hmm1'          : hmm1_name,
                        'hmm2'          : hmm2_name,
                        'links'         : len(c),
                        'hmm1_skew'     : hmm1_skew,
                        'hmm2_skew'     : hmm2_skew,
                        'hmm1_kurtosis' : hmm1_kurtosis,
                        'hmm2_kurtosis' : hmm2_kurtosis,
                        'hmm1_stdev'    : hmm1_stdev,
                        'hmm2_stdev'    : hmm2_stdev,
                        'r'             : r,
                        'pr'            : pr,
                        'tau'           : t,
                        'p_tau'         : pt,
                        'cluster'       : int( params.cid ),
                        'ANI'           : float( params.ani) } )
      
      LOG.write( 'writing records for {n} correlations...\n'.format( n=len( data ) ) )
      polars.DataFrame( data ).write_csv( file=output[0], sep='\t' )

rule clusterlinks :
  input :
    treeA = 'trees/by_gene/{hmm1}.nwk',
    treeB = 'trees/by_gene/{hmm2}.nwk',
    ani_clusters = expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS )
  output :
    'cluster_correlations/{hmm1}_{hmm2}.tsv'
  params :
    min_cluster_links = MIN_CLUSTER_LINKS
  log :
    'logs/cluster_correlations/{hmm1}_{hmm2}.log'
  run :
    
    gene1_name = Path( input.treeA ).stem.rsplit( '.', 1 )[0]
    gene2_name = Path( input.treeB ).stem.rsplit( '.', 1 )[0]
    
    T1 = SuchTree( input.treeA )
    T2 = SuchTree( input.treeB )
    links = find_links( T1, T2 )
    
    data = []
    for ani_cluster_file in input.ani_clusters :
      
      threshold  = Path( ani_cluster_file ).stem.rsplit( '_', 1 )[1]
      
      with open( ani_cluster_file ) as f :
        ani_clusters = { cid : c.split(',') for cid,c in [ line.split('\t') for line in f ] }
        
      cluster_links = defaultdict(list)
      for key in ani_clusters.keys() :
        for a,b in links :
          a_genome = a.rsplit('_',3)[0]
          b_genome = b.rsplit('_',3)[0]
          if a_genome in ani_clusters[key] and b_genome in ani_clusters[key] :
            cluster_links[key].append( ( a, b ) )
      
      for cid,c in cluster_links.items() :
        if len(c) > params.min_cluster_links :
            gene1_pairs = []
            gene2_pairs = []
            
            for (t1a,t2a),(t1b,t2b) in combinations( c, 2 ) :
                gene1_pairs.append( ( t1a, t1b ) )
                gene2_pairs.append( ( t2a, t2b ) )
            
            gene1_distances = T1.distances_by_name( gene1_pairs )
            gene2_distances = T2.distances_by_name( gene2_pairs )
            
            # don't include degenerate clusters
            if len( unique( gene1_distances ) ) / len( gene1_distances ) < 0.5 : continue
            if len( unique( gene2_distances ) ) / len( gene2_distances ) < 0.5 : continue
            
            gene1_skew = skew( gene1_distances )
            gene2_skew = skew( gene2_distances )
            
            gene1_kurtosis = kurtosis( gene1_distances )
            gene2_kurtosis = kurtosis( gene2_distances )
            
            gene1_stdev = stdev( gene1_distances )
            gene2_stdev = stdev( gene2_distances )
            
            r,pr = pearsonr(   gene1_distances, gene2_distances )
            t,pt = kendalltau( gene1_distances, gene2_distances )
            
            data.append(  { 'gene1'          : gene1_name,
                            'gene2'          : gene2_name,
                            'links'          : len(c),
                            'gene1_skew'     : gene1_skew,
                            'gene2_skew'     : gene2_skew,
                            'gene1_kurtosis' : gene1_kurtosis,
                            'gene2_kurtosis' : gene2_kurtosis,
                            'gene1_stdev'    : gene1_stdev,
                            'gene2_stdev'    : gene2_stdev,
                            'r'              : r,
                            'pr'             : pr,
                            'tau'            : t,
                            'p_tau'          : pt,
                            'cluster'        : cid,
                            'ANI'            : threshold } )
    
    pandas.DataFrame( data ).to_csv( output[0], sep='\t', index=False )

rule merge_cluster_correlations :
  input :
    tsvs = expand( 'cluster_correlations/{hmm1}_{hmm2}.tsv', zip, hmm1=HMMS1, hmm2=HMMS2 )
  output :
    'cluster_correlations/cluster_correlations.tsv'
  log :
    'logs/cluster_correlations/cluster_correlations.log'
  run :
    with open( log[0], 'w' ) as logfile :
      data = []
      for fname in input.tsvs :
        logfile.write( 'reading {fname}...'.format( fname=fname ) )
        try :
          data.append( pandas.read_csv( fname, sep='\t' ) )
        except pandas.errors.EmptyDataError :
          pass
      df = pandas.concat( data, ignore_index=True )
      df.to_csv( output[0], sep='\t', index=False )


