include : 'Importer'

# load configuration
configfile : 'config/config.yaml'

from utils import parse_hmmsearch
from utils import bipartition_compatibility, bipartition_compatibility_ratio
from utils import find_links
from utils import batched

import os, pandas, polars, dendropy
from pathlib import Path
from copy import deepcopy
from hmm_profile import reader as hmmreader
from hmm_profile import writer as hmmwriter
from cdhit_reader import read_cdhit
from Bio.SeqIO import parse
from Bio import AlignIO, SearchIO
from Bio.Align import MultipleSeqAlignment
from numpy import linspace, zeros, unique
from collections import defaultdict
from itertools import combinations, product
from scipy.cluster.hierarchy import single, fcluster
from scipy.spatial.distance import squareform
from scipy.stats import kendalltau, pearsonr, skew, kurtosis
from statistics import stdev
from SuchTree import SuchTree, SuchLinkedTrees

HMMS_DIR    = config['hmm_dir']
GENOMES_DIR = config['genome_dir']

# index data inputs
HMM_FILES    = glob_wildcards( os.path.join( HMMS_DIR, '{hmm}.hmm' ) )
GENOME_FILES = glob_wildcards( os.path.join( GENOMES_DIR, '{fasta}.fna' ) )

# aggregate genomes into chunks
CHUNKS = [ 'chunk_{i}'.format(i=str(i)) for i in range( config['chunks'] ) ]

# extract the {hmm} values into a list
HMMS    = HMM_FILES.hmm

# extract the {fasta} values into a list
GENOMES = GENOME_FILES.fasta

# genome fasta paths
GENOME_PATHS = { genome : os.path.join( GENOMES_DIR, genome + '.fna' ) 
                 for genome in GENOMES }

# reference and query set for fastANI
R = GENOMES[:len(GENOMES)//2]
Q = GENOMES[len(GENOMES)//2:]

# gene pair lists
HMMS1,HMMS2 = list( zip( *combinations( HMMS, 2 ) ) )

# e-value threshold for HMM search
EVALUE = config['evalue']

# post-trimming aligned fraction threshold
AFT = config['aligned_fraction_threshold']

# threads to use when invoking multithreaded applications
THREADS = config['threads']

ANI_THREADS = config['ani_threads']

# cutoff thresholds for genome ANI clustering
ANI_THRESHOLDS = linspace( config['ani_thresholds_min'],
                           config['ani_thresholds_max'],
                           config['ani_thresholds_n'] )

# minimum number of links per cluster for tree correlation
MIN_CLUSTER_LINKS = config['min_cluster_links']

# degenerate tree threshold : exclude pairwise tree-to-tree
# comparisons where the ratio of non-unique leaf-to-leaf
# distances exceeds this threshold
DEGEN_THRESHOLD = config['degeneracy_threshold']

rule all :
  input :
    'cluster_data/clusters.tsv'

rule clean :
  shell :
    'rm -rf aggregated_genomes proteins genes \
        hmm_hits hmmdb preani ani ani_clusters \
        scored_proteins cluster_data clusters \
        statistics benchmarks logs'

rule aggregate :
  group : 'gene_prediction'
  input :
    fastas=expand( 'data/genomes-enabled/{fasta}.fna', fasta=GENOMES ),
  output :
    chunks=expand( 'aggregated_genomes/{chunk}.fna', chunk=CHUNKS ),
  log :
    'logs/aggregate/aggregate.log',
  run :
    with open( log[0], 'w' ) as LOG :
      fasta_chunks = batched( input.fastas, size=len(input.fastas)//config['chunks'] )
      if not os.path.exists( 'aggregated_genomes' ) : os.mkdir( 'aggregated_genomes' )
      for n,chunk in enumerate( fasta_chunks ) :
        chunkpath = os.path.join( 'aggregated_genomes',
                                  'chunk_{n}.fna'.format(n=str(n) ) ) 
        LOG.write( 'writing {chunk}...\n'.format( chunk=chunkpath ) )
        with open( chunkpath, 'w' ) as f :
          contigs = 0
          for i,genome in enumerate( chunk ) :
            for j,seq in enumerate( parse( open( genome ), 'fasta' ) ) :
              seq.id = Path(genome).stem + '_contig_' + str(j)
              seq.description = ''
              f.write( seq.format('fasta') )
            contigs = contigs + j
          LOG.write( '   wrote {i} genomes with {j} contigs\n'.format( i=str(i+1),
                                                                       j=str(contigs+1) ) )

rule prodigal :
  group : 'gene_prediction'
  input :
    fna='aggregated_genomes/{chunk}.fna', 
  output :
    faa='proteins/{chunk}.faa',
    fna='genes/{chunk}.fna',
  log :
    'logs/prodigal/{chunk}.log',
  benchmark :
    'benchmarks/prodigal/{chunk}.tsv'
  params :
    extra='',
  wrapper :
    'file://wrappers/prodigal-gv'

rule hmmdb :
  group : 'gene_prediction'
  input :
    hmms = expand( 'data/hmms-enabled/{hmm}.hmm', hmm=HMMS )
  output :
    db   = 'hmmdb/models.hmm'
  log :
    'logs/hmmdb/hmmdb.log'
  benchmark :
    'benchmarks/hmmdb/hmmdb.tsv'
  run :
    profiles = []
    for hmmfile in input.hmms :
      hmm = hmmreader.read_single( open( hmmfile ) )
      if Path(hmmfile).stem != hmm.metadata.model_name :
        raise Exception( 'file name ({hmmfile}) must match model name ({modelname}).'.format(
                          hmmfile=hmmfile, modelname=hmm.metadata.model_name ) )
      profiles.append( hmm ) 
    hmmwriter.save_many_to_file( hmms=profiles, output=output.db )

rule hmmer :
  group : 'gene_prediction'
  input :
    fasta   = 'proteins/{chunk}.faa',
    profile = 'hmmdb/models.hmm'
  log :
    'logs/hmmer/{chunk}.log',
  benchmark :
    'benchmarks/hmmer/{chunk}.tsv'
  output :
    tblout         = 'hmm_hits/{chunk}_prot_tbl.txt',
    #domtblout      = 'hmm_hits/{fasta}_{hmm}_prot_dom_tbl.txt',
    #alignment_hits = 'hmm_hits/{fasta}_{hmm}_alg_hits.txt',
    #outfile        = 'hmm_hits/{fasta}_{hmm}_prot.txt'
  wrapper :
    'file://wrappers/hmmer/hmmsearch'

rule scoring :
  input :
    # for each HMM...
    hits    = expand( 'hmm_hits/{chunk}_prot_tbl.txt', chunk=CHUNKS ),
    # ... score orthologs for each genome...
    faa     = expand( 'proteins/{chunk}.faa', chunk=CHUNKS ),
    profile = 'hmmdb/models.hmm'
  output :
    prealign = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS )
  log :
    'logs/scoring/scoring.log'
  benchmark :
    'benchmarks/scoring/scoring.tsv'
  run :
    with open( log[0], 'w' ) as LOG :
      prealignments = { Path(p).stem : { 'prealign' : p,
                                         'hits'     : [] } for p in output.prealign }
      # use trusted cutoffs if provided
      trusted_cutoffs = { m.metadata.model_name : m.metadata.trusted_cutoff
                          for m in hmmreader.read_all( open( input.profile ) ) }
      LOG.write( 'parsed {n} hmms, found {m} trusted cutoffs\n'.format( 
                    n=str(len(trusted_cutoffs)),
                    m=str(sum( [ bool(v) for v in trusted_cutoffs.values() ] ) ) ) )
      # load hmmer hits and protein sequences for each chunk
      for tblout,faa in zip( input.hits, input.faa ) :
        hits = polars.DataFrame( [ h for h in parse_hmmsearch( tblout ) ] )
        seqs = { seq.id : seq for seq in parse( open( faa ), 'fasta' ) }
        chunk_genomes = { name.rsplit('_',1)[0] for name in seqs.keys() }
        LOG.write( '{chunk} : found {h} hits for {p} proteins in {g} genomes\n'.format(
                      chunk=Path(faa).stem,
                      h=str(len(hits)),
                      p=str(len(seqs)),
                      g=str(len(chunk_genomes)) ) )
        if len( hits ) == 0 : continue
        # for each hmm, for each genome, write the proteins 
        # with the top hits in order of their score value
        for genome,hmm in product( hits['tname'].unique(),
                                   hits['qname'].unique() ) :
          if trusted_cutoffs[ hmm ] :
            top_hits = hits.filter( ( polars.col('tname') == genome ) &
                                    ( polars.col('qname') == hmm ) &
                                    ( polars.col('score') >= trusted_cutoff[hmm] ) )\
                           .sort( 'score', descending=True )
          else :
            top_hits = hits.filter( ( polars.col('tname') == genome ) &
                                    ( polars.col('qname') == hmm ) &
                                    ( polars.col('eval') <= EVALUE ) )\
                           .sort( 'score', descending=True )
          if len( top_hits ) == 0 : continue
          with open( prealignments[ hmm ]['prealign'], 'a' ) as f :
            for n,protein in enumerate( top_hits['tname'] ) :
              seq = deepcopy( seqs[ protein ] )
              if seq.seq[-1] == '*' : seq = seq[:-1]
              seq.id = seq.id + '_p' + str(n)
              prealignments[hmm]['hits'].append( seq.id )
              f.write( seq.format( 'fasta' ) )
      for k,p in prealignments.items() :
        genomes = { h.rsplit('_',4)[0] for h in p['hits'] }
        LOG.write( '{hmm} : \n'.format( hmm=k ) )
        LOG.write( '   genomes : {n}\n'.format( n=str(len(genomes)) ) )
        LOG.write( '   hits    : {n}\n'.format( n=str(len(p['hits'])) ) )
        if len( p['hits'] ) == 0 :
          LOG.write( '    NONE\n' )

rule preani :
  group : 'ani'
  input :
    references = expand( 'data/genomes-enabled/{reference}.fna', reference=R ),
    queries    = expand( 'data/genomes-enabled/{query}.fna', query=Q )
  output :
    references = 'preani/references.txt',
    queries    = 'preani/queries.txt',
  log :
    'logs/preani/preani.log'
  benchmark :
    'benchmarks/preani/preani.tsv'
  run :
    with open( output.references, 'w' ) as rf :
      for fasta in input.references :
        rf.write( fasta + '\n' )
    with open( output.queries, 'w' ) as qf :
      for fasta in input.queries :
        qf.write( fasta + '\n' )

rule fastani :
  group : 'ani'
  input :
    reference = 'preani/references.txt',
    query     = 'preani/queries.txt'
  output :
    'ani/ani.tsv'
  log :
    'logs/fastani/fastani.log'
  benchmark :
    'benchmarks/fastani/fastani.tsv'
  threads : ANI_THREADS
  wrapper :
    'file://wrappers/fastani'

rule ani_clusters :
  group : 'ani'
  input :
    ani = 'ani/ani.tsv',
  output :
    clusterfiles = expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS )
  params :
    min_ani = config['ani_thresholds_min'],
    max_ani = config['ani_thresholds_max'],
    min_aln = config['ani_min_aln'],
    ani_cutoffs = ANI_THRESHOLDS,
  log :
    'logs/ani_clusters/ani_clusters.log'
  benchmark :
    'benchmarks/ani_clusters/ani_cutoffs.txt',
  run :
    with open( log[0], 'w' ) as LOG :
      header = [ 'query', 'reference', 'ANI', 
                 'bidirectional fragment mappings',
                 'total query fragments' ]
      
      fastani = polars.read_csv( '../kizuchi/ani/ani.tsv', separator='\t', has_header=False )
      fastani.columns = header
      
      LOG.write( 'found {n} ANI hits.\n'.format( n=str(len(fastani)) ) ) 
      
      # file names -> genome names
      fastani = fastani.with_columns( fastani.map_rows( lambda t : Path(t[0]).stem )['map'].alias('query') )
      fastani = fastani.with_columns( fastani.map_rows( lambda t : Path(t[1]).stem )['map'].alias('reference') )
      
      # %ID -> distance
      fastani = fastani.with_columns( fastani.select( 1 - polars.col('ANI') / 100 )['literal'].alias('distance') )
      
      # drop NaNs
      fastani = fastani.filter( polars.all_horizontal( polars.selectors.float().is_not_nan() ) )
      
      filteredani = fastani.filter( ( polars.col('ANI') <= params.max_ani )
                                  & ( polars.col('ANI') >= params.min_ani )
                                  & ( polars.col('bidirectional fragment mappings') >= params.min_aln ) )
      
      LOG.write( 'found {n} ANI hits that meet filtering criteria.\n'.format( n=str(len(filteredani)) ) )
      
      # make some indexes
      genomes = list( set( filteredani['query'] ) | set( filteredani['reference'] ) )
      genome_index = { g:n for n,g in enumerate( genomes ) }
      
      # allocate the empty matrix
      D = zeros( ( len(genomes), len(genomes) ) )
      D.fill( 1.0 )
      
      # set self-distance to zero
      for i in range(len(genomes)) :
        D[i,i] = 0.0
      
      # populate distance matrix with filtered ANI hits
      for query,reference,ani,bfm,tqf,d in filteredani.iter_rows() :
        i = genome_index[ query ]
        j = genome_index[ reference ]
        D[i,j] = D[j,i] = d
      
      # single linkage clustering at each ANI threshold
      all_clusters = {}
      for T in params.ani_cutoffs :
        threshold = 1 - (  T / 100 )
        ani_clusters = defaultdict(list)
        for n,c in enumerate( fcluster( single( squareform(D) ),
                              threshold,
                              criterion='distance' ) ) :
          ani_clusters[c].append( genomes[n] )
        LOG.write( 'found {n} clusters at ANI>{t}.\n'.format( n=str(n), t=str(T) ) )
        all_clusters[T] = ani_clusters
      
      # write genome ANI cluster files
      outfiles = { t : p for t,p in zip( params.ani_cutoffs, output.clusterfiles ) }
      LOG.write( outfiles )
      for ani_cutoff,ani_clusters in all_clusters.items() :
        with open( outfiles[ani_cutoff], 'w' ) as f :
          for cid,cluster_genomes in ani_clusters.items() :
            f.write( '{cid}\t{genomes}\n'.format( cid=str(cid),
                                                  genomes=','.join(cluster_genomes) ) )


rule build_clusters :
  input :
    prealignments = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS ),
    genes         = expand( 'genes/{chunk}.fna', chunk=CHUNKS ),
    ani_clusters  = expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS )
  output :
    cluster_table = 'cluster_data/clusters.tsv',
    clusters      = directory( 'clusters' ),
  params :
    ANIs = ANI_THRESHOLDS
  log :
    'logs/build_clusters/build_clusters.log'
  benchmark :
    'benchmarks/cluster_filtering/cluster_filtering.tsv'
  run :
    def intersect( hmms, genomes, threshold ) :
      C = defaultdict( set )
      for hmm,genome in zip( hmms, genomes ) :
        C[hmm].add(genome)
      return sum( [ len( C[a] & C[b] ) > threshold
                    for a,b in combinations( C.keys(), 2 ) ] )
    
    with open( log[0], 'w' ) as LOG :
      LOG.write( 'loading sequences...\n' )
      
      sequences = { Path(fasta).stem : { rec.id : rec.seq
                                         for rec
                                         in parse( open( fasta ), 'fasta' ) } 
                    for fasta
                    in input.prealignments }
      
      LOG.write( 'Genes loaded   : {n}\n'.format( n=str(len(sequences)) ) )
      
      geneseqs = defaultdict(dict)
      for chunk in input.genes :
        for seq in parse( open( chunk ), 'fasta' ) :
          genome = seq.id.rsplit('_',3)[0] 
          geneseqs[ genome ][ seq.id ] = seq
      
      LOG.write( 'Genomes loaded : {n}\n'.format( n=str(len(geneseqs)) ) )
      
      LOG.write( 'creating gene table for genome clusters...\n' )
      
      records = []
      for ani,clusterfile in zip( params.ANIs, input.ani_clusters ) :
        for cid,names in [ line.split() for line in open( clusterfile ) ] :
          names = names.split(',')
          #if not len( names ) > 5 : continue
          for hmm in sequences.keys() :
            seq_ids = [ seq_id for seq_id in sequences[hmm].keys() if seq_id.rsplit('_',4)[0] in names ]
            for seq_id in seq_ids :
              records.append( { 'seq_id' : seq_id,
                                'genome' : seq_id.rsplit('_',4)[0],
                                'cid'    : int( cid ),
                                'hmm'    : hmm,
                                'ANI'    : float( ani ),
                                'id'     : cid + '_' + Path( clusterfile ).stem.split('_')[-1] } )
      
      df = polars.DataFrame( records )
      
      LOG.write( 'number of clusters by ANI...\n' )
      for ani,n in df.groupby('ANI').agg( polars.col('cid').unique().count() ).sort('ANI').iter_rows() :
        LOG.write( '    ANI={ani} : {n} clusters\n'.format( ani=ani, n=n ) )
      
      LOG.write( 'gene table description :\n\n' + str( df.describe() ) + '\n' )
      
      clusters = df.groupby('id').agg( ['hmm','genome'] ).apply(
                         lambda x : ( x[0],
                           intersect( x[1],
                                      x[2],
                                      MIN_CLUSTER_LINKS ) ) ).filter( polars.col('column_1') > 0 )['column_0']
      
      LOG.write( 'found {n} genome clusters with at least {T} genomes sharing at least two genes.\n'.format(
                 n=len(clusters), T=MIN_CLUSTER_LINKS ) )
      
      cluster_table = df.filter( polars.col('id').is_in( clusters ) )
      
      LOG.write( 'filtered gene table description :\n\n' + str( cluster_table.describe() ) )

      cluster_table.write_csv( file=output.cluster_table, separator='\t' )

      # create cluster directory structure and write gene nucleotide
      # sequences as individual fastas per cluster
      
      LOG.write( 'found {n} clusters.\n'.format( n=len( cluster_table.groupby('id').agg('seq_id') ) ) )
      
      if not os.path.exists( 'clusters' ) : os.mkdir( 'clusters' )

      for ANI,cids in cluster_table.groupby('ANI').agg( polars.col('cid').unique() ).sort('ANI').iter_rows() :
        
        anipath = os.path.join( 'clusters', str(ANI) )
        if not os.path.exists( anipath ) : os.mkdir( anipath )
        
        for cluster_id in cids :
          
          clusterpath = os.path.join( anipath, str( cluster_id ) )
          if not os.path.exists( clusterpath ) : os.mkdir( clusterpath )
          genepath = os.path.join( clusterpath, 'genes' )
          if not os.path.exists( genepath ) : os.mkdir( genepath )
          
          for hmm,seq_ids,genomes in cluster_table.filter( ( polars.col('ANI') == ANI ) & ( polars.col('cid') == cluster_id ) ).groupby('hmm').agg( ['seq_id', 'genome'] ).iter_rows() :
            with open( os.path.join( genepath, hmm + '.fna' ), 'w' ) as f :
              for seq_id,genome in zip( seq_ids, genomes ) :
                orig_seq_id = seq_id.rsplit('_',1)[0] # remove hmmer evalue rank
                seq = geneseqs[genome][orig_seq_id] 
                seq.id = seq_id
                f.write( seq.format( 'fasta' ) )
