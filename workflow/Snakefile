import os, pandas, dendropy
from pathlib import Path
from copy import deepcopy
from hmm_profile import reader, writer
from cdhit_reader import read_cdhit
from Bio.SeqIO import parse
from Bio import AlignIO
from Bio.Align import MultipleSeqAlignment
from numpy import linspace, zeros
from collections import defaultdict
from scipy.cluster.hierarchy import single, fcluster
from scipy.spatial.distance import squareform

FILES1 = glob_wildcards( 'data/hmms-enabled/{hmm}.hmm' )
FILES2 = glob_wildcards( 'data/genomes-enabled/{fasta}.fna' )

# extract the {hmm} values into a list
HMMS    = FILES1.hmm

# extract the {fasta} values into a list
GENOMES = FILES2.fasta

# reference and query set for fastANI
R = GENOMES[:len(GENOMES)//2]
Q = GENOMES[len(GENOMES)//2:]

# e-value threshold for HMM search
EVALUE = 10e-3

# post-trimming aligned fraction threshold
AFT = 0.75

# threads to use when invoking multithreaded applications
THREADS=8

# cutoff thresholds for gene clustering
GENE_OTU_THRESHOLDS = [ 0.7, 0.8, 0.9, 0.93, 0.95, 0.98, 0.99 ]

# cutoff thresholds for genome ANI clustering
ANI_THRESHOLDS = linspace( 75, 99, 25 )

# borrowed from CheckV
def parse_hmmsearch( path ) :
  with open(path) as f:
    names = [ 'tname', 'qacc', 'qname', 'tacc',   'eval', 
              'score', 'bias', 'beval', 'bscore', 'bbias' ]
    formats = [ str,   str,   str,   str,   float, 
                float, float, float, float, float ]
    for line in f:
      if not line.startswith( '#' ) :
        values = line.split()
        yield dict( [ ( names[i], formats[i](values[i]) ) 
                      for i in range(10) ] )

def find_links( T1, T2 ) :
    '''
    For two SuchTree objects representing two gene trees from the same set of
    genomes, find the links connecting leaf nodes in each gene tree.
    '''
    t1_genomes = { name : name.rsplit('_', 3)[0] for name in T1.leafs.keys() }
    t2_genomes = { name : name.rsplit('_', 3)[0] for name in T2.leafs.keys() }
    
    t1_genes = defaultdict(list)
    for name in T1.leafs.keys() :
        genome = name.rsplit( '_', 3 )[0]
        t1_genes[genome].append( name )
    
    t2_genes = defaultdict(list)
    for name in T2.leafs.keys() :
        genome = name.rsplit( '_', 3 )[0]
        t2_genes[genome].append( name )
    
    links = []
    for genome in set( t1_genes.keys() ) & set( t2_genes.keys() ) :
        
        # if both genes are single copy a genome, it is linked
        # if either gene has duplications, each copy of each
        # gene is linked only if they are on the same contig
        
        if len( t1_genes[ genome] ) == len( t2_genes[ genome ] ) == 1 :
            
            links.append( ( t1_genes[genome][0], t2_genes[genome][0] ) )
            
        else :
            for gene1 in t1_genes[ genome ] :
                for gene2 in t2_genes[ genome ] :
                    contig1 = gene1.rsplit('_', 2)[0]
                    contig2 = gene2.rsplit('_', 2)[0]
                    if contig1 == contig2 :
                        links.append( ( gene1, gene2 ) )
    
    return links

rule all :
  input :
    expand( 'trees/by_taxa/{hmm}.nwk', hmm=HMMS ),
    expand( 'gene_clusters/{gene_cutoff}/{hmm}.tsv', gene_cutoff=GENE_OTU_THRESHOLDS, hmm=HMMS ),
    expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS ),
    'statistics/summary.txt'

rule clean :
  shell :
    'rm -rf proteins hmm_hits hmmdb \
        prealignments\ alignments \
        trimmed_alignments\
        filtered_trimmed_alignments\
        trees statistics logs\
        precluster_sequences clusters\
        preani ani'

rule statistics :
  input :
    hmms       = expand( 'data/hmms-enabled/{hmm}.hmm', hmm=HMMS ),
    genomes    = expand( 'data/genomes-enabled/{fasta}.fna', fasta=GENOMES ),
    proteins   = expand( 'proteins/{fasta}_genes.faa', fasta=GENOMES ),
    hits       = expand( 'hmm_hits/{fasta}_prot_tbl.txt', hmm=HMMS, fasta=GENOMES ),
    alignments = expand( 'trimmed_alignments/{hmm}.msa.faa', hmm=HMMS ),
    trees      = expand( 'trees/by_taxa/{hmm}.nwk', hmm=HMMS )
  output :
    'statistics/summary.txt'
  log :
    notebook='logs/notebooks/statistics.ipynb',
  notebook :
    'notebooks/statistics.ipynb'

rule prodigal :
  input :
    fna='data/genomes-enabled/{fasta}.fna', 
  output :
    faa='proteins/{fasta}_genes.faa',
    #fna='proteins/{fasta}_genes.fna',
    #gff='proteins/{fasta}_genes.gff',
  log :
    'logs/prodigal/{fasta}.log',
  params :
    extra='',  # Additional arguments
  wrapper :
    'file://wrappers/prodigal-gv'

rule hmmdb :
  input :
    hmms = expand( 'data/hmms-enabled/{hmm}.hmm', hmm=HMMS )
  output :
    db   = 'hmmdb/models.hmm'
  log :
    'logs/hmmdb/hmmdb.log'
  run :
    profiles = []
    for hmmfile in input.hmms :
      hmm = reader.read_single( open( hmmfile ) )
      if Path(hmmfile).stem != hmm.metadata.model_name :
        raise Exception( 'file name ({hmmfile}) must match model name ({modelname}).'.format(
                          hmmfile=hmmfile, modelname=hmm.metadata.model_name ) )
      profiles.append( hmm ) 
    writer.save_many_to_file( hmms=profiles, output=output.db )

rule hmmer :
  input :
    fasta   = 'proteins/{fasta}_genes.faa',
    profile = 'hmmdb/models.hmm'
  log :
    'logs/hmmer/{fasta}.log',
  output :
    tblout         = 'hmm_hits/{fasta}_prot_tbl.txt',
    #domtblout      = 'hmm_hits/{fasta}_{hmm}_prot_dom_tbl.txt',
    #alignment_hits = 'hmm_hits/{fasta}_{hmm}_alg_hits.txt',
    #outfile        = 'hmm_hits/{fasta}_{hmm}_prot.txt'
  wrapper :
    'file://wrappers/hmmer/hmmsearch'

rule scoring :
  input :
    # for each HMM...
    hits    = expand( 'hmm_hits/{fasta}_prot_tbl.txt', fasta=GENOMES ),
    # ...score orthologs for each genome
    faa     = expand( 'proteins/{fasta}_genes.faa', fasta=GENOMES ),
    profile = 'hmmdb/models.hmm'
  output :
    prealign = expand( 'prealignments/{hmm}.faa', hmm=HMMS )
  log :
    'logs/scoring/scoring.log'
  run :
    with open( log[0], 'w' ) as LOG :
      prealignments = { Path(p).stem : { 'prealign' : p,
                                         'hits'     : [] } for p in output.prealign }
      # use trusted cutoffs if provided
      trusted_cutoffs = { m.metadata.model_name : m.metadata.trusted_cutoff
                          for m in reader.read_all( open( input.profile ) ) }
      LOG.write( 'parsed {n} hmms, found {m} trusted cutoffs\n'.format( 
                    n=str(len(trusted_cutoffs)),
                    m=str(sum( [ bool(v) for v in trusted_cutoffs.values() ] ) ) ) )
      for tblout,faa in zip( input.hits, input.faa ) :
        hits = pandas.DataFrame( [ h for h in parse_hmmsearch( tblout ) ] )
        seqs = { seq.id : seq for seq in parse( open( faa ), 'fasta' ) }
        genome = Path(faa).stem.rsplit('_',1)[0]
        LOG.write( '{genome} : found {n} hits for {m} proteins\n'.format(
                      genome=genome,
                      n=str(len(hits)),
                      m=str(len(seqs)) ) )
        if len( hits ) == 0 : continue
        for hmm in set( hits['qname'] ) :
          if trusted_cutoffs[ hmm ] :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['score'] >= trusted_cutoff[hmm] ) ].sort_values( 
                                'score', axis=0, ascending=False )
          else :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['eval']  <= EVALUE ) ].sort_values( 
                                'score', axis=0, ascending=False )
          if len( top_hits ) == 0 : continue
          with open( prealignments[ hmm ]['prealign'], 'a' ) as f :
            for i,(n,row) in enumerate( top_hits.iterrows() ) :
              LOG.write( '   {hmm} {n} : {tname}\n'.format( hmm=hmm, n=str(i), tname=row['tname'] ) )
              seq = deepcopy( seqs[ row['tname'] ] )
              if seq.seq[-1] == '*' : seq = seq[:-1]
              seq.id = seq.id + '_p' + str(i)
              prealignments[hmm]['hits'].append( seq.id )
              f.write( seq.format( 'fasta' ) )
      for k,p in prealignments.items() :
        LOG.write( '{hmm} : \n'.format( hmm=k ) )
        for hit in p['hits'] :
          LOG.write( '    {hit}\n'.format( hit=hit ) )
        if len( p['hits'] ) == 0 :
          LOG.write( '    NONE\n' )
          with open( p['prealign'], 'w' ) as f :
            f.write( '' )

rule clustalo :
  input :
    'prealignments/{hmm}.faa'
  output :
    'alignments/{hmm}.msa.faa'
  params:
    extra=''
  log :
    'logs/clustalo/{hmm}.log'
  threads : THREADS
  wrapper :
    'file://wrappers/clustalo'

rule trimal :
  input :
    'alignments/{hmm}.msa.faa'
  output :
    'trimmed_alignments/{hmm}.msa.faa'
  log :
    'logs/trimal/{hmm}.log'
  params :
    extra='-automated1'
  wrapper :
    'file://wrappers/trimal'

rule filter_alignments :
  input :
    'trimmed_alignments/{hmm}.msa.faa'
  output :
    'filtered_trimmed_alignments/{hmm}.msa.faa'
  log :
    'logs/filter_alignments/{hmm}.log'
  run :
    if open( input[0] ).read().count( '>' ) < 3 :
      with open( log[0], 'w' ) as f :
        f.write( 'no records.' )
      with open( output[0], 'w' ) as f :
        f.write( '' )
    else :
      A = AlignIO.read( input[0], 'fasta' )
      L = A.get_alignment_length()
      B = MultipleSeqAlignment( [ s for s in A if len(s.seq.ungap())/L > AFT ] )
      with open( log[0], 'w' ) as f :
        f.write( 'aligned fraction threshold : {n}\n'.format( n=AFT ) )
        f.write( 'alignment length           : {n}\n'.format( n=L ) )
        f.write( 'input alignment sequences  : {n}\n'.format( n=len(A) ) )
        f.write( 'output alignment sequences : {n}\n'.format( n=len(B) ) )
      with open( output[0], 'w' ) as f :
        if len(B) >= 3 :
          AlignIO.write( B, f, 'fasta' )
        else :
          f.write( '' )

rule precluster :
  input :
    'filtered_trimmed_alignments/{hmm}.msa.faa'
  output :
    'precluster_sequences/{hmm}.faa'
  log :
    'logs/precluster/{hmm}.log'
  run :
    with open( output[0], 'w' ) as f :
      for rec in parse( open( input[0] ), 'fasta' ) :
        rec.seq = rec.seq.replace( '-', '' )
        f.write( rec.format( 'fasta' ) )

rule gene_cluster :
  input :
    'precluster_sequences/{hmm}.faa'
  output :
    clusters = 'gene_clusters/{gene_cutoff}/{hmm}.faa.clstr',
    rep_seqs = 'gene_clusters/{gene_cutoff}/{hmm}.faa'
  log :
    'logs/gene_cluster/{gene_cutoff}_{hmm}.log'
  params :
    cutoff   = '{gene_cutoff}'
  wrapper :
    'file://wrappers/cd-hit'

rule postcluster :
  input :
    'gene_clusters/{gene_cutoff}/{hmm}.faa.clstr'
  output :
    'gene_clusters/{gene_cutoff}/{hmm}.tsv'
  log :
    'logs/postcluster/{gene_cutoff}_{hmm}.log'
  run :
    with open( output[0], 'w' ) as f :
      for cluster in read_cdhit( input[0] ) :
        f.write( '\t'.join( [ s.name for s in cluster.sequences ] ) + '\n' )

rule fasttree :
  input :
    alignment='filtered_trimmed_alignments/{hmm}.msa.faa'
  output :
    tree='trees/by_gene/{hmm}.nwk'
  log :
    'logs/fasttree/{hmm}.log'
  params :
    extra=''
  wrapper :
    'file://wrappers/fasttree'

rule taxatrees :
  '''
  Prune trees to individual taxa using. Taxa are represented
  by the gene with the top-scoring HMM hit.
  '''
  input :
    'trees/by_gene/{hmm}.nwk',
  output :
    'trees/by_taxa/{hmm}.nwk',
  log :
    'logs/gene-taxa/{hmm}_mapping.txt',
  run :
    with open( log[0], 'w' ) as LOG :
      if not os.path.getsize( input[0] ) == 0 :
        T = dendropy.Tree.get( path=input[0],
                               schema='newick',
                               preserve_underscores=True )
        drop = [ leaf.taxon.label for leaf in T.leaf_nodes() 
                 if leaf.taxon.label.rsplit('_', 1)[1] != 'p0' ]
        T.prune_taxa_with_labels( drop )
        for leaf in T.leaf_nodes() :
          taxon = leaf.taxon.label.rsplit( '_', 3 )[0]
          LOG.write( '{a}\t{b}\n'.format( a=leaf.taxon.label, b=taxon ) )
          leaf.taxon.label = taxon
        T.write( path=output[0],
                 schema='newick' )
      else :
        open( output[0], 'w' ).close()

rule preani :
  input :
    references = expand( 'data/genomes-enabled/{reference}.fna', reference=R ),
    queries    = expand( 'data/genomes-enabled/{query}.fna', query=Q )
  output :
    references = 'preani/references.txt',
    queries    = 'preani/queries.txt',
  log :
    'logs/preani/preani.log'
  run :
    with open( output.references, 'w' ) as rf :
      for fasta in input.references :
        rf.write( fasta + '\n' )
    with open( output.queries, 'w' ) as qf :
      for fasta in input.queries :
        qf.write( fasta + '\n' )

rule fastani :
  input :
    reference = 'preani/references.txt',
    query     = 'preani/queries.txt'
  output :
    'ani/ani.tsv'
  log :
    'logs/fastani/fastani.log'
  threads : THREADS
  wrapper :
    'file://wrappers/fastani'

rule ani_clusters :
  input :
    ani = 'ani/ani.tsv',
  output :
    'ani_clusters/ani_clusters_{ani_cutoff}.txt'
  log :
    'log/ani_clusters/{ani_cutoff}.log'
  run :
    header = [ 'query', 'reference', 'ANI', 
               'bidirectional fragment mappings',
               'total query fragments' ]
    
    fastani = pandas.read_csv( input.ani, sep='\t', names=header )
    
    # file names -> genome names
    fastani['query']     = [ Path(p).stem for p in fastani['query'] ]
    fastani['reference'] = [ Path(p).stem for p in fastani['reference'] ]
    
    # drop self-hits
    fastani = fastani[ fastani['query'] != fastani['reference'] ]
    
    # de-duplicate (i.e., take the lower triangle of the matrix)
    fastani['hash'] = fastani.apply( lambda row : hash( 
                                                    tuple(
                                                      sorted( ( row['query'],
                                                                row['reference'] ) ) ) ), 
                                    axis=1 )

    fastani.drop_duplicates( subset='hash', keep='first', inplace=True )
    fastani.drop( ['hash'], axis=1, inplace=True )
    
    # put the 
    threshold = 1.0 - float( Path( output[0] ).stem.rsplit( '_', 1 )[1] )/100

    with open( output[0], 'w' ) as f :
      genomes = list( set( fastani['query'] ) | set( fastani['reference'] ) )
    
      D = zeros( ( len(genomes), len(genomes) ) )
      D.fill( 1.0 )
        
      for i in range(len(genomes)) :
        D[i,i] = 0.0
        
      for n,row in fastani.iterrows() :
        i = genomes.index( row['query'] )
        j = genomes.index( row['reference'] )
        D[i,j] = D[j,i] = 1.0 - row['ANI']/100
         
      ani_clusters = defaultdict(list)
      for n,c in enumerate( fcluster( single( squareform(D) ),
                                      threshold,
                                      criterion='distance' ) ) :
        ani_clusters[c].append( genomes[n] )
        
      for cid, cg in ani_clusters.items() :
        f.write( '{cid}\t{genomes}\n'.format( cid=str(cid), genomes=','.join(cg) ) )
