# load configuration
configfile : 'config/config.yaml'

import os, pandas, polars, dendropy
from pathlib import Path
from copy import deepcopy
from hmm_profile import reader, writer
from cdhit_reader import read_cdhit
from Bio.SeqIO import parse
from Bio import AlignIO, SearchIO
from Bio.Align import MultipleSeqAlignment
from numpy import linspace, zeros, unique
from collections import defaultdict
from itertools import combinations, product
from scipy.cluster.hierarchy import single, fcluster
from scipy.spatial.distance import squareform
from scipy.stats import kendalltau, pearsonr, skew, kurtosis
from statistics import stdev
from SuchTree import SuchTree, SuchLinkedTrees

HMMS_DIR    = 'data/hmms-enabled'
GENOMES_DIR = 'data/genomes-enabled'

# index data inputs
HMM_FILES    = glob_wildcards( os.path.join( HMMS_DIR, '{hmm}.hmm' ) )
GENOME_FILES = glob_wildcards( os.path.join( GENOMES_DIR, '{fasta}.fna' ) )

# extract the {hmm} values into a list
HMMS    = HMM_FILES.hmm

# extract the {fasta} values into a list
GENOMES = GENOME_FILES.fasta

# genome fasta paths
GENOME_PATHS = { genome : os.path.join( GENOMES_DIR, genome + '.fna' ) 
                 for genome in GENOMES }

# reference and query set for fastANI
R = GENOMES[:len(GENOMES)//2]
Q = GENOMES[len(GENOMES)//2:]

# gene pair lists
HMMS1,HMMS2 = list( zip( *combinations( HMMS, 2 ) ) )

# cluster checkpoint wildcards
#CKANIS = None
#CKCIDS = None
#CKHMMS = None

# e-value threshold for HMM search
EVALUE = 10e-3

# post-trimming aligned fraction threshold
AFT = 0.75

# threads to use when invoking multithreaded applications
THREADS=1

# cutoff thresholds for genome ANI clustering
ANI_THRESHOLDS = linspace( 75, 99, 25 )

# minimum number of links per cluster for tree correlation
MIN_CLUSTER_LINKS = 5

# degenerate tree threshold : exclude pairwise tree-to-tree
# comparisons where the ratio of non-unique leaf-to-leaf
# distances exceeds this threshold
DEGEN_THRESHOLD = 0.5

# borrowed from CheckV
def parse_hmmsearch( path ) :
  with open(path) as f:
    names = [ 'tname', 'qacc', 'qname', 'tacc',   'eval', 
              'score', 'bias', 'beval', 'bscore', 'bbias' ]
    formats = [ str,   str,   str,   str,   float, 
                float, float, float, float, float ]
    for line in f:
      if not line.startswith( '#' ) :
        values = line.split()
        yield dict( [ ( names[i], formats[i](values[i]) ) 
                      for i in range(10) ] )

def bipartition_compatibility( A, B ) :
    '''
    Test for compatibility of two bipartitions.
    
    [1] Salichos, Leonidas, Alexandros Stamatakis, and
        Antonis Rokas. "Novel information theory-based 
        measures for quantifying incongruence among 
        phylogenetic trees." Molecular biology and 
        evolution 31, no. 5 (2014): 1261-1271.
    [2] Salichos, Leonidas, and Antonis Rokas. "Inferring
        ancient divergences requires genes with strong 
        phylogenetic signals." Nature 497, no. 7449 
        (2013): 327-331.
    '''
    A0,A1 = A
    B0,B1 = B
    return ( not bool( A0 & B0 ) ) \
         | ( not bool( A1 & B0 ) ) \
         | ( not bool( A0 & B1 ) ) \
         | ( not bool( A1 & B1 ) )

def bipartition_compatibility_ratio( T1, T2, links ) :
    '''
    Compute the ratio of compatible bipartitions of connected
    leaf nodes between two gene trees. Connected leaf names 
    are mapped into a common namespace, so only bipartitions
    of the connected leaf nodes are included in the ratio.
    '''
    L1,L2 = zip(*links)
    
    B1 = frozenset( frozenset((a,b)) for a,b in 
                    [ [ frozenset( L1.index(i) for i in a if i in L1 ), 
                        frozenset( L1.index(i) for i in b if i in L1 ) ]
                      for a,b in T1.bipartitions() ]
                    if a and b )
    
    B2 = frozenset( frozenset((a,b)) for a,b in 
                    [ [ frozenset( L2.index(i) for i in a if i in L2 ), 
                        frozenset( L2.index(i) for i in b if i in L2 ) ]
                      for a,b in T2.bipartitions() ]
                    if a and b )
    
    S = [ bipartition_compatibility( b1, b2 ) for b1,b2 in product( B1, B2 ) ]
    
    return { 'ratio'   : sum(S)/len(S),
             'jaccard' : len( B1 & B2 ) / len( B1 | B2 ) }

def find_links( genes1, genes2 ) :
    '''
    For two sets of genes called from the same set of genomes, find the linkage
    relationships connecting the genes.
    '''
    t1_genomes = { name : name.rsplit('_', 3)[0] for name in genes1 }
    t2_genomes = { name : name.rsplit('_', 3)[0] for name in genes2 }
    
    t1_genes = defaultdict(list)
    for name in genes1 :
        genome = name.rsplit( '_', 3 )[0]
        t1_genes[genome].append( name )
    
    t2_genes = defaultdict(list)
    for name in genes2 :
        genome = name.rsplit( '_', 3 )[0]
        t2_genes[genome].append( name )
    
    links = []
    for genome in set( t1_genes.keys() ) & set( t2_genes.keys() ) :
        
        # if both genes are single copy a genome, it is linked
        # if either gene has duplications, each copy of each
        # gene is linked only if they are on the same contig
        
        if len( t1_genes[ genome] ) == len( t2_genes[ genome ] ) == 1 :
            
            links.append( ( t1_genes[genome][0], t2_genes[genome][0] ) )
            
        else :
            for gene1 in t1_genes[ genome ] :
                for gene2 in t2_genes[ genome ] :
                    contig1 = gene1.rsplit('_', 2)[0]
                    contig2 = gene2.rsplit('_', 2)[0]
                    if contig1 == contig2 :
                        links.append( ( gene1, gene2 ) )
    
    return links

def get_cluster_trees( wildcards ) :

  ck_output = checkpoints.write_cluster_fastas.get(**wildcards).output[0]
  
  CK = glob_wildcards( os.path.join( ck_output,
                                     wildcards.ani,
                                     wildcards.cid,
                                     'genes',
                                     '{ckhmm}.fna' ) )
  
  return expand( os.path.join( ck_output,
                               wildcards.ani,
                               wildcards.cid,
                               'trees',
                               'by_gene',
                               '{ckhmm}.nwk' ),
                 ckhmm=CK.ckhmm )
  
def get_cluster_data( wildcards ) :
  
  ck_output = checkpoints.write_cluster_fastas.get(**wildcards).output[0]
  
  CK = glob_wildcards( os.path.join( ck_output,
                                     '{ani}',
                                     '{cid}',
                                     'genes',
                                     '{ckhmm}.fna' ) )
  
  return expand( os.path.join( ck_output,
                               '{ani}',
                               '{cid}',
                               'data.tsv' ),
                 zip,
                 ani=CK.ani,
                 cid=CK.cid )
  
rule all :
  input :
    #expand( 'trees/by_taxa/{hmm}.nwk', hmm=HMMS ),
    #expand( 'gene_clusters/{gene_cutoff}/{hmm}.tsv', gene_cutoff=GENE_OTU_THRESHOLDS, hmm=HMMS ),
    #expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS ),
    #expand( 'cluster_correlations/{hmm1}_{hmm2}.tsv', zip, hmm1=HMMS ),
    #'statistics/hmmr_summary.txt'
    #'cluster_correlations/cluster_correlations.tsv',
    #expand( 'clusters/{ani}/{cid}/alignments/{hmm}_aln.faa', 
    #        zip, ani=CK.ani, cid=CK.cid, hmm=CK.hmm )
    #get_cluster_filenames
    get_cluster_data

rule clean :
  shell :
    'rm -rf proteins genes hmm_hits hmmdb \
        preani ani ani_clusters scored_proteins \
        cluster_filtering clusters statistics logs'

rule hmmr_summary :
  input :
    genomes    = expand( 'data/genomes-enabled/{fasta}.fna', fasta=GENOMES ),
    proteins   = expand( 'proteins/{fasta}_genes.faa', fasta=GENOMES ),
    hits       = expand( 'hmm_hits/{fasta}_prot_tbl.txt', hmm=HMMS, fasta=GENOMES ),
  output :
    'statistics/hmmr_summary.txt'
  log :
    'logs/statistics/hmmr_summary.log'
  run :
    attribs = [ 'accession', 'bias', 'bitscore', 'description',
                'cluster_num', 'domain_exp_num', 'domain_included_num',
                'domain_obs_num', 'domain_reported_num', 'env_num',
                'evalue', 'id', 'overlap_num', 'region_num']
    
    hitlist = {}
    
    for hmm_hits in input.hits :
      hits = defaultdict(list)
      with open( hmm_hits ) as handle :
        for queryresult in SearchIO.parse( handle, 'hmmer3-tab' ) :
          for hit in queryresult.hits :
            for attrib in attribs :
              hits[attrib].append( getattr(hit, attrib) )
            hitlist[hit.id] = { 'hmm'      : queryresult.id,
                                'bitscore' : hit.bitscore }
    
    # Note : counting proteins from 1 instead of 0 because that's how
    # prodigal does it. Other counts are from 0.
    
    with open( output[0], 'w' ) as f :
      for fna,faa in list( zip( input.genomes, input.proteins ) ) :
        f.write( fna + '\n')
        contigs  = [ rec.id for rec in parse( open( fna ), 'fasta' ) ]
        proteins = [ rec.id for rec in parse( open( faa ), 'fasta' ) ]
        annot = {}
        for r in proteins :
          contig = r.rsplit('_', 1)[0]
          if not contig in annot :
            annot[contig] = []
          annot[contig].append( r )
        for n,contig in enumerate( contigs ) :
          f.write( '   contig {n} : {c} '.format( n=n, c=contig ) + '\n' )
          for m,protein in enumerate( annot[contig] ) :
            if protein in hitlist :
              f.write( '      protien {m} : {p}\n'.format( m=m+1, p=protein ) )
              f.write( '         hmm : {h} bitscore {b}\n'.format( h=hitlist[protein]['hmm'],
                                                                   b=hitlist[protein]['bitscore'] ) )
            else :
              f.write( '      protien {m} : {p}\n'.format( m=m+1, p=protein ) )


rule prodigal :
  input :
    fna='data/genomes-enabled/{fasta}.fna', 
  output :
    faa='proteins/{fasta}_genes.faa',
    fna='genes/{fasta}_genes.fna',
    #gff='proteins/{fasta}_genes.gff',
  log :
    'logs/prodigal/{fasta}.log',
  benchmark :
    'benchmarks/prodigal/{fasta}.tsv'
  params :
    extra='',
  wrapper :
    'file://wrappers/prodigal-gv'

rule hmmdb :
  input :
    hmms = expand( 'data/hmms-enabled/{hmm}.hmm', hmm=HMMS )
  output :
    db   = 'hmmdb/models.hmm'
  log :
    'logs/hmmdb/hmmdb.log'
  benchmark :
    'benchmarks/hmmdb/hmmdb.tsv'
  run :
    profiles = []
    for hmmfile in input.hmms :
      hmm = reader.read_single( open( hmmfile ) )
      if Path(hmmfile).stem != hmm.metadata.model_name :
        raise Exception( 'file name ({hmmfile}) must match model name ({modelname}).'.format(
                          hmmfile=hmmfile, modelname=hmm.metadata.model_name ) )
      profiles.append( hmm ) 
    writer.save_many_to_file( hmms=profiles, output=output.db )

rule hmmer :
  input :
    fasta   = 'proteins/{fasta}_genes.faa',
    profile = 'hmmdb/models.hmm'
  log :
    'logs/hmmer/{fasta}.log',
  benchmark :
    'benchmarks/hmmer/{fasta}.tsv'
  output :
    tblout         = 'hmm_hits/{fasta}_prot_tbl.txt',
    #domtblout      = 'hmm_hits/{fasta}_{hmm}_prot_dom_tbl.txt',
    #alignment_hits = 'hmm_hits/{fasta}_{hmm}_alg_hits.txt',
    #outfile        = 'hmm_hits/{fasta}_{hmm}_prot.txt'
  wrapper :
    'file://wrappers/hmmer/hmmsearch'

rule scoring :
  input :
    # for each HMM...
    hits    = expand( 'hmm_hits/{fasta}_prot_tbl.txt', fasta=GENOMES ),
    # ... score orthologs for each genome...
    faa     = expand( 'proteins/{fasta}_genes.faa', fasta=GENOMES ),
    profile = 'hmmdb/models.hmm'
  output :
    prealign = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS )
  log :
    'logs/scoring/scoring.log'
  benchmark :
    'benchmarks/scoring/scoring.tsv'
  run :
    with open( log[0], 'w' ) as LOG :
      prealignments = { Path(p).stem : { 'prealign' : p,
                                         'hits'     : [] } for p in output.prealign }
      # use trusted cutoffs if provided
      trusted_cutoffs = { m.metadata.model_name : m.metadata.trusted_cutoff
                          for m in reader.read_all( open( input.profile ) ) }
      LOG.write( 'parsed {n} hmms, found {m} trusted cutoffs\n'.format( 
                    n=str(len(trusted_cutoffs)),
                    m=str(sum( [ bool(v) for v in trusted_cutoffs.values() ] ) ) ) )
      for tblout,faa in zip( input.hits, input.faa ) :
        hits = pandas.DataFrame( [ h for h in parse_hmmsearch( tblout ) ] )
        seqs = { seq.id : seq for seq in parse( open( faa ), 'fasta' ) }
        genome = Path(faa).stem.rsplit('_',1)[0]
        LOG.write( '{genome} : found {n} hits for {m} proteins\n'.format(
                      genome=genome,
                      n=str(len(hits)),
                      m=str(len(seqs)) ) )
        if len( hits ) == 0 : continue
        for hmm in set( hits['qname'] ) :
          if trusted_cutoffs[ hmm ] :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['score'] >= trusted_cutoff[hmm] ) ].sort_values( 
                                'score', axis=0, ascending=False )
          else :
            top_hits = hits[ ( hits['qname'] == hmm ) &
                             ( hits['eval']  <= EVALUE ) ].sort_values( 
                                'score', axis=0, ascending=False )
          if len( top_hits ) == 0 : continue
          with open( prealignments[ hmm ]['prealign'], 'a' ) as f :
            for i,(n,row) in enumerate( top_hits.iterrows() ) :
              LOG.write( '   {hmm} {n} : {tname}\n'.format( hmm=hmm, n=str(i), tname=row['tname'] ) )
              seq = deepcopy( seqs[ row['tname'] ] )
              if seq.seq[-1] == '*' : seq = seq[:-1]
              seq.id = seq.id + '_p' + str(i)
              prealignments[hmm]['hits'].append( seq.id )
              f.write( seq.format( 'fasta' ) )
      for k,p in prealignments.items() :
        LOG.write( '{hmm} : \n'.format( hmm=k ) )
        for hit in p['hits'] :
          LOG.write( '    {hit}\n'.format( hit=hit ) )
        if len( p['hits'] ) == 0 :
          LOG.write( '    NONE\n' )
          with open( p['prealign'], 'w' ) as f :
            f.write( '' )

rule preani :
  input :
    references = expand( 'data/genomes-enabled/{reference}.fna', reference=R ),
    queries    = expand( 'data/genomes-enabled/{query}.fna', query=Q )
  output :
    references = 'preani/references.txt',
    queries    = 'preani/queries.txt',
  log :
    'logs/preani/preani.log'
  benchmark :
    'benchmarks/preani/preani.tsv'
  run :
    with open( output.references, 'w' ) as rf :
      for fasta in input.references :
        rf.write( fasta + '\n' )
    with open( output.queries, 'w' ) as qf :
      for fasta in input.queries :
        qf.write( fasta + '\n' )

rule fastani :
  input :
    reference = 'preani/references.txt',
    query     = 'preani/queries.txt'
  output :
    'ani/ani.tsv'
  log :
    'logs/fastani/fastani.log'
  benchmark :
    'benchmarks/fastani/fastani.tsv'
  threads : THREADS
  wrapper :
    'file://wrappers/fastani'

rule ani_clusters :
  input :
    ani = 'ani/ani.tsv',
  output :
    'ani_clusters/ani_clusters_{ani_cutoff}.txt'
  log :
    'logs/ani_clusters/{ani_cutoff}.log'
  benchmark :
    'benchmarks/ani_clusters/{ani_cutoff}.tsv'
  run :
    header = [ 'query', 'reference', 'ANI', 
               'bidirectional fragment mappings',
               'total query fragments' ]
    
    fastani = pandas.read_csv( input.ani, sep='\t', names=header )
    
    # file names -> genome names
    fastani['query']     = [ Path(p).stem for p in fastani['query'] ]
    fastani['reference'] = [ Path(p).stem for p in fastani['reference'] ]
    
    # drop self-hits
    fastani = fastani[ fastani['query'] != fastani['reference'] ]
    
    # de-duplicate (i.e., take the lower triangle of the matrix)
    fastani['hash'] = fastani.apply( lambda row : hash( 
                                                    tuple(
                                                      sorted( ( row['query'],
                                                                row['reference'] ) ) ) ), 
                                    axis=1 )

    fastani.drop_duplicates( subset='hash', keep='first', inplace=True )
    fastani.drop( ['hash'], axis=1, inplace=True )
    
    # convert % identity to a distance measure for clustering
    threshold = 1.0 - float( Path( output[0] ).stem.rsplit( '_', 1 )[1] )/100
    
    with open( output[0], 'w' ) as f :
      genomes = list( set( fastani['query'] ) | set( fastani['reference'] ) )
    
      D = zeros( ( len(genomes), len(genomes) ) )
      D.fill( 1.0 )
        
      for i in range(len(genomes)) :
        D[i,i] = 0.0
        
      for n,row in fastani.iterrows() :
        i = genomes.index( row['query'] )
        j = genomes.index( row['reference'] )
        D[i,j] = D[j,i] = 1.0 - row['ANI']/100
         
      ani_clusters = defaultdict(list)
      for n,c in enumerate( fcluster( single( squareform(D) ),
                                      threshold,
                                      criterion='distance' ) ) :
        ani_clusters[c].append( genomes[n] )
        
      for cid, cg in ani_clusters.items() :
        f.write( '{cid}\t{genomes}\n'.format( cid=str(cid), genomes=','.join(cg) ) )


rule cluster_filtering :
  input :
    prealignments = expand( 'scored_proteins/{hmm}.faa', hmm=HMMS ),
    ani_clusters  = expand( 'ani_clusters/ani_clusters_{ani_cutoff}.txt', ani_cutoff=ANI_THRESHOLDS )
  output :
    clusters = 'cluster_filtering/clusters.tsv'
  params :
  log :
    'logs/cluster_filtering/cluster_filtering.log'
  benchmark :
    'benchmarks/cluster_filtering/cluster_filtering.tsv'
  run :
    def intersect( hmms, genomes, threshold ) :
      C = defaultdict( set )
      for hmm,genome in zip( hmms, genomes ) :
        C[hmm].add(genome)
      return sum( [ len( C[a] & C[b] ) > threshold
                    for a,b in combinations( C.keys(), 2 ) ] )
    
    with open( log[0], 'w' ) as LOG :
      LOG.write( 'loading sequences...\n' )
      sequences = { Path(fasta).stem : { rec.id : rec.seq for rec in parse( open( fasta ), 'fasta' ) } 
                    for fasta in input.prealignments }
      
      for hmm in sequences.keys() :
        LOG.write( '{hmm} : {n}\n'.format( hmm=hmm, n=len(sequences[hmm]) ) )
      
      LOG.write( 'creating gene table for genome clusters...\n' )
      records = []
      for clusterfile in input.ani_clusters :
        for cid,names in [ line.split() for line in open( clusterfile ) ] :
          names = names.split(',')
          #if not len( names ) > 5 : continue
          for hmm in sequences.keys() :
            seq_ids = [ seq_id for seq_id in sequences[hmm].keys() if seq_id.rsplit('_',3)[0] in names ]
            for seq_id in seq_ids :
              records.append( { 'seq_id' : seq_id,
                                'genome' : seq_id.rsplit('_',3)[0],
                                'cid'    : int( cid ),
                                'hmm'    : hmm,
                                'ANI'    : float( Path( clusterfile ).stem.split('_')[-1] ),
                                'id'     : cid + '_' + Path( clusterfile ).stem.split('_')[-1] } )
      
      df = polars.DataFrame( records )
      
      LOG.write( 'number of clusters by ANI...\n' )
      for ani,n in df.groupby('ANI').agg( polars.col('cid').unique().count() ).sort('ANI').iter_rows() :
        LOG.write( '    ANI={ani} : {n} clusters\n'.format( ani=ani, n=n ) )
      
      LOG.write( 'gene table description :\n\n' + str( df.describe() ) + '\n' )
      
      clusters = df.groupby('id').agg( ['hmm','genome'] ).apply(
                         lambda x : ( x[0],
                           intersect( x[1],
                                      x[2],
                                      MIN_CLUSTER_LINKS ) ) ).filter( polars.col('column_1') > 0 )['column_0']
      
      LOG.write( 'found {n} genome clusters with at least {T} genomes sharing at least two genes.\n'.format(
                 n=len(clusters), T=MIN_CLUSTER_LINKS ) )
      
      cluster_table = df.filter( polars.col('id').is_in( clusters ) )
      
      LOG.write( 'filtered gene table description :\n\n' + str( cluster_table.describe() ) )

      cluster_table.write_csv( file=output.clusters, separator='\t' )

checkpoint write_cluster_fastas :
  input :
    cluster_table = 'cluster_filtering/clusters.tsv',
    genes         = expand( 'genes/{fasta}_genes.fna', fasta=GENOMES )
  output :
    clusters = directory( 'clusters' )
  log :
    'logs/write_cluster_fastas/write_cluster_fastas.log'
  benchmark :
    'benchmarks/write_cluster_fastas/write_cluster_fastas.tsv'
  run :
    with open( log[0], 'w' ) as LOG :
      LOG.write( 'loading sequences...\n' )
      geneseqs = { Path(fasta).stem.rsplit('_',1)[0] : { rec.id : rec
                                        for rec in parse( open( fasta ), 'fasta' ) }
                   for fasta in input.genes }
      
      for genome in geneseqs.keys() :
        LOG.write( '{genome} : {n}\n'.format( genome=genome, n=len(geneseqs[genome]) ) )
 
      LOG.write( 'loading cluster table...\n' )
      df = polars.read_csv( input.cluster_table, separator='\t' )
      LOG.write( 'found {n} clusters.\n'.format( n=len( df.groupby('id').agg('seq_id') ) ) )

      if not os.path.exists( 'clusters' ) : os.mkdir( 'clusters' )
      for ANI,cids in df.groupby('ANI').agg( polars.col('cid').unique() ).sort('ANI').iter_rows() :
        anipath = os.path.join( 'clusters', str(ANI) )
        if not os.path.exists( anipath) : os.mkdir( anipath )
        for cluster_id in cids :
          clusterpath = os.path.join( anipath, str( cluster_id ) )
          if not os.path.exists( clusterpath ) : os.mkdir( clusterpath )
          genepath = os.path.join( clusterpath, 'genes' )
          if not os.path.exists( genepath ) : os.mkdir( genepath )
          hmms_written = set() # FIXME : workaround for post-checkpoint wildcard matching
          for hmm,seq_ids,genomes in df.filter( ( polars.col('ANI') == ANI ) & ( polars.col('cid') == cluster_id ) ).groupby('hmm').agg( ['seq_id', 'genome'] ).iter_rows() :
            hmms_written.add( hmm ) # FIXME : workaround for post-checkpoint wildcard matching
            with open( os.path.join( genepath, hmm + '.fna' ), 'w' ) as f :
              for seq_id,genome in zip( seq_ids, genomes ) :
                orig_seq_id = seq_id.rsplit('_',1)[0] # remove hmmer evalue rank
                seq = geneseqs[genome][orig_seq_id] 
                seq.id = seq_id
                f.write( seq.format( 'fasta' ) )

rule clustalo :
  input :
    'clusters/{ani}/{cid}/genes/{ckhmm}.fna'
  output :
    'clusters/{ani}/{cid}/alignments/{ckhmm}_aln.fna'
  params :
    extra=''
  log :
    'logs/clustalo/{ani}_{cid}_{ckhmm}.log'
  benchmark :
    'benchmarks/clustalo/{ani}_{cid}_{ckhmm}.tsv'
  threads : THREADS
  wrapper :
    'file://wrappers/clustalo'

rule trimal :
  input :
    'clusters/{ani}/{cid}/alignments/{ckhmm}_aln.fna'
  output :
    'clusters/{ani}/{cid}/trimmed_alignments/{ckhmm}_aln.fna'
  log :
    'logs/trimal/{ani}_{cid}_{ckhmm}.log'
  benchmark :
    'benchmarks/trimal/{ani}_{cid}_{ckhmm}.tsv'
  params :
    extra='-automated1'
  wrapper :
    'file://wrappers/trimal'

rule filter_alignments :
  input :
    'clusters/{ani}/{cid}/trimmed_alignments/{ckhmm}_aln.fna'
  output :
    'clusters/{ani}/{cid}/filtered_trimmed_alignments/{ckhmm}_aln.fna'
  log :
    'logs/filter_alignments/{ani}_{cid}_{ckhmm}.log'
  benchmark :
    'benchmarks/filter_alignments/{ani}_{cid}_{ckhmm}.tsv'
  run :
    if open( input[0] ).read().count( '>' ) < 3 :
      with open( log[0], 'w' ) as f :
        f.write( 'no records.' )
      with open( output[0], 'w' ) as f :
        f.write( '' )
    else :
      A = AlignIO.read( input[0], 'fasta' )
      L = A.get_alignment_length()
      B = MultipleSeqAlignment( [ s for s in A if len(s.seq.replace('-',''))/L > AFT ] )
      with open( log[0], 'w' ) as f :
        f.write( 'aligned fraction threshold : {n}\n'.format( n=AFT ) )
        f.write( 'alignment length           : {n}\n'.format( n=L ) )
        f.write( 'input alignment sequences  : {n}\n'.format( n=len(A) ) )
        f.write( 'output alignment sequences : {n}\n'.format( n=len(B) ) )
      with open( output[0], 'w' ) as f :
        if len(B) >= 3 :
          AlignIO.write( B, f, 'fasta' )
        else :
          f.write( '' )

rule fasttree :
  input :
    alignment = 'clusters/{ani}/{cid}/filtered_trimmed_alignments/{ckhmm}_aln.fna'
  output :
    tree = 'clusters/{ani}/{cid}/trees/by_gene/{ckhmm}.nwk'
  log :
    'logs/fasttree/{ani}_{cid}_{ckhmm}.log'
  benchmark :
    'benchmarks/fasttree/{ani}_{cid}_{ckhmm}.tsv'
  params :
    extra=''
  wrapper :
    'file://wrappers/fasttree'

rule taxatrees :
  '''
  Prune trees to individual taxa using. Taxa are represented
  by the gene with the top-scoring HMM hit.
  '''
  input :
    'clusters/{ani}/{cid}/trees/by_gene/{ckhmm}.nwk',
  output :
    'clusters/{ani}/{cid}/trees/by_taxa/{ckhmm}.nwk',
  log :
    'logs/gene-taxa/{ani}_{cid}_{ckhmm}_mapping.txt',
  benchmark :
    'benchmarks/taxatrees/{ani}_{cid}_{ckhmm}.tsv'
  run :
    with open( log[0], 'w' ) as LOG :
      if not os.path.getsize( input[0] ) == 0 :
        T = dendropy.Tree.get( path=input[0],
                               schema='newick',
                               preserve_underscores=True )
        
        # Sometimes, the paralog with the lowest e-value from hmmer
        # doesn't make it through trimming and filtering. The paralog
        # with the lowest hmmer e-value AFTER alignment, trimming and
        # filtering is chosen to represent the genome in the taxa tree.
        
        G = defaultdict(dict)
        
        for leaf in T.leaf_nodes() :
          genome,contig,gene,paralog = leaf.taxon.label.rsplit( '_', 3 )
          G[genome][int(paralog[1:])] = leaf.taxon.label
        
        keep = [ G[genome][min(G[genome].keys())] for genome in G.keys() ]
        
        T.retain_taxa_with_labels( keep )

        for leaf in T.leaf_nodes() :
          taxon = leaf.taxon.label.rsplit( '_', 3 )[0]
          LOG.write( '{a}\t{b}\n'.format( a=leaf.taxon.label, b=taxon ) )
          leaf.taxon.label = taxon
        T.write( path=output[0],
                 schema='newick' )
      else :
        open( output[0], 'w' ).close()

rule clusterdata :
  input :
    trees = get_cluster_trees
  output :
    table = 'clusters/{ani}/{cid}/data.tsv'
  params :
    ani   = '{ani}',
    cid   = '{cid}',
  log :
    'logs/clusterdata/{ani}_{cid}.log'
  benchmark :
    'benchmarks/clusterdata/{ani}_{cid}.tsv'
  run :
    with open( log[0], 'w' ) as LOG :
       
      LOG.write( 'ANI        : {ani}\n'.format( ani=params.ani ) )
      LOG.write( 'cluster ID : {cid}\n'.format( cid=params.cid ) )
      
      LOG.write( 'input trees :\n' )
      for treefile in input.trees :
        LOG.write( '    {f}\n'.format( f=treefile ) )
      
      data = []
      
      for treefile1, treefile2 in combinations( input.trees, 2 ) :
        
        # if either treefile is an empty sentilel file, skip the calculations
        if os.stat( treefile1 ).st_size == 0 : continue
        if os.stat( treefile2 ).st_size == 0 : continue
        
        hmm1_name = Path( treefile1 ).stem.rsplit( '.', 1 )[0]
        hmm2_name = Path( treefile2 ).stem.rsplit( '.', 1 )[0]
        
        LOG.write( 'loading {g1} and {g2}...\n'.format( g1=hmm1_name, g2=hmm2_name ) )
        
        T1 = SuchTree( treefile1 )
        T2 = SuchTree( treefile2 )
        links = find_links( list( T1.leafs.keys() ), list( T2.leafs.keys() ) )
        
        LOG.write( '    {g1} : {n} leafs\n'.format( g1=hmm1_name, n=T1.n_leafs ) )
        LOG.write( '    {g2} : {n} leafs\n'.format( g2=hmm2_name, n=T2.n_leafs ) )
        LOG.write( '    links : {n}\n'.format( n=len(links) ) )
        
        # some clusters may have lost enough members during trimming and filtering
        # that they no longer meet the link threshold
        if len(links) < MIN_CLUSTER_LINKS :
          LOG.write( 'gene pair is below minimum link threshold; aborting.\n' )
          continue
        
        hmm1_pairs = []
        hmm2_pairs = []
        
        for (t1a,t2a),(t1b,t2b) in combinations( links, 2 ) :
          hmm1_pairs.append( ( t1a, t1b ) )
          hmm2_pairs.append( ( t2a, t2b ) )
        
        hmm1_distances = T1.distances_by_name( hmm1_pairs )
        hmm2_distances = T2.distances_by_name( hmm2_pairs )
        
        LOG.write( '    computed {n} distances.\n'.format( n=str( len( hmm1_distances ) ) ) )
        
        # skip degenerate trees
        if len( unique( hmm1_distances ) ) / len( hmm1_distances ) < DEGEN_THRESHOLD :
          LOG.write( '    {g1} is degenerate, skipping.\n'.format( g1=hmm1_name ) )
          continue
        if len( unique( hmm2_distances ) ) / len( hmm2_distances ) < DEGEN_THRESHOLD :
          LOG.write( '    {g2} is degenerate, skipping.\n'.format( g2=hmm2_name ) )
          continue
        
        hmm1_skew = skew( hmm1_distances )
        hmm2_skew = skew( hmm2_distances )
        
        hmm1_kurtosis = kurtosis( hmm1_distances )
        hmm2_kurtosis = kurtosis( hmm2_distances )
         
        hmm1_stdev = stdev( hmm1_distances )
        hmm2_stdev = stdev( hmm2_distances )
         
        r,pr = pearsonr(   hmm1_distances, hmm2_distances )
        t,pt = kendalltau( hmm1_distances, hmm2_distances )
        
        bp_compat = bipartition_compatibility_ratio( T1, T2, links )
        
        data.append(  { 'hmm1'          : hmm1_name,
                        'hmm2'          : hmm2_name,
                        'hmm1_leafs'    : T1.n_leafs,
                        'hmm2_leafs'    : T2.n_leafs,
                        'links'         : len(links),
                        'hmm1_skew'     : hmm1_skew,
                        'hmm2_skew'     : hmm2_skew,
                        'hmm1_kurtosis' : hmm1_kurtosis,
                        'hmm2_kurtosis' : hmm2_kurtosis,
                        'hmm1_stdev'    : hmm1_stdev,
                        'hmm2_stdev'    : hmm2_stdev,
                        'bp_ratio'      : bp_compat['ratio'],
                        'bp_jaccard'    : bp_compat['jaccard'],
                        'r'             : r,
                        'pr'            : pr,
                        'tau'           : t,
                        'p_tau'         : pt,
                        'cluster'       : int( params.cid ),
                        'ANI'           : float( params.ani) } )
      
      if data :
        LOG.write( 'writing records for {n} correlations...\n'.format( n=len( data ) ) )
        polars.DataFrame( data ).write_csv( file=output[0], separator='\t' )
      else :
        LOG.write( 'no gene pairs exceed minimum link threshold, aborting.\n' )
